{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "        pathway = os.path.join(dirname, filename)\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "from IPython.display import HTML\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "\n",
    "manualSeed = 999\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "torch.use_deterministic_algorithms(True) # Needed for reproducible results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data (path)\n",
    "dataset_name = 'gan-getting-started'\n",
    "root = '../input/'+dataset_name\n",
    "\n",
    "# data (img)\n",
    "img_height = 256\n",
    "img_width = 256\n",
    "channels = 3\n",
    "sample_interval = 1\n",
    "\n",
    "epoch = 0 \n",
    "n_epochs = 200 # number of epochs of training\n",
    "batch_size = 1 # size of the batches\n",
    "lr = 0.0002 # adam : learning rate\n",
    "b1 = 0.5 # adam : decay of first order momentum of gradient\n",
    "b2 = 0.999 # adam : decay of first order momentum of gradient\n",
    "decay_epoch = 10 # suggested default : 100 (suggested 'n_epochs' is 200)\n",
    "                 # epoch from which to start lr decay\n",
    "    \n",
    "# Lists to store losses for visualization\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "\n",
    "n_cpu = 1 # number of cpu threads to use during batch generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        self.block = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1), # Pads the input tensor using the reflection of the input boundary\n",
    "            nn.Conv2d(in_features, in_features, 3),\n",
    "            nn.InstanceNorm2d(in_features), \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_features, in_features, 3),\n",
    "            nn.InstanceNorm2d(in_features)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorResNet(nn.Module):\n",
    "    def __init__(self, input_shape, num_residual_block):\n",
    "        super(GeneratorResNet, self).__init__()\n",
    "        \n",
    "        channels = input_shape[0]\n",
    "        \n",
    "        # Initial Convolution Block\n",
    "        out_features = 64\n",
    "        model = [\n",
    "            nn.ReflectionPad2d(channels),\n",
    "            nn.Conv2d(channels, out_features, 7),\n",
    "            nn.InstanceNorm2d(out_features),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ]\n",
    "        in_features = out_features\n",
    "        \n",
    "        # Downsampling\n",
    "        for _ in range(2):\n",
    "            out_features *= 2\n",
    "            model += [\n",
    "                nn.Conv2d(in_features, out_features, 3, stride=2, padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ]\n",
    "            in_features = out_features\n",
    "        \n",
    "        # Residual blocks\n",
    "        for _ in range(num_residual_block):\n",
    "            model += [ResidualBlock(out_features)]\n",
    "            \n",
    "        # Upsampling\n",
    "        for _ in range(2):\n",
    "            out_features //= 2\n",
    "            model += [\n",
    "                nn.Upsample(scale_factor=2), # --> width*2, heigh*2\n",
    "                nn.Conv2d(in_features, out_features, 3, stride=1, padding=1),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ]\n",
    "            in_features = out_features\n",
    "            \n",
    "        # Output Layer\n",
    "        model += [nn.ReflectionPad2d(channels),\n",
    "                  nn.Conv2d(out_features, channels, 7),\n",
    "                  nn.Tanh()\n",
    "                 ]\n",
    "        \n",
    "        # Unpacking\n",
    "        self.model = nn.Sequential(*model) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "#     TEST CODE : nn.Upsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        channels, height, width = input_shape\n",
    "        \n",
    "        # Calculate output shape of image discriminator (PatchGAN)\n",
    "        self.output_shape = (1, height//2**4, width//2**4)\n",
    "        \n",
    "        def discriminator_block(in_filters, out_filters, normalize=True):\n",
    "            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n",
    "            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n",
    "            if normalize:\n",
    "                layers.append(nn.InstanceNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(channels, 64, normalize=False),\n",
    "            *discriminator_block(64, 128),\n",
    "            *discriminator_block(128,256),\n",
    "            *discriminator_block(256,512),\n",
    "            nn.ZeroPad2d((1,0,1,0)),\n",
    "            nn.Conv2d(512, 1, 4, padding=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, img):\n",
    "        return self.model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_m = nn.ZeroPad2d((0,0,0,1)) # Check Padding Function\n",
    "# test_m(tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Loss Functions\n",
    "criterion_GAN = torch.nn.MSELoss()\n",
    "criterion_cycle = torch.nn.L1Loss()\n",
    "criterion_identity = torch.nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (channels, img_height, img_width) # (3,256,256)\n",
    "n_residual_blocks = 9 # suggested default, number of residual blocks in generator\n",
    "\n",
    "G_AB = GeneratorResNet(input_shape, n_residual_blocks)\n",
    "G_BA = GeneratorResNet(input_shape, n_residual_blocks)\n",
    "D_A = Discriminator(input_shape)\n",
    "D_B = Discriminator(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02) # reset Conv2d's weight(tensor) with Gaussian Distribution\n",
    "        if hasattr(m, 'bias') and m.bias is not None:\n",
    "            torch.nn.init.constant_(m.bias.data, 0.0) # reset Conv2d's bias(tensor) with Constant(0)\n",
    "        elif classname.find('BatchNorm2d') != -1:\n",
    "            torch.nn.init.normal_(m.weight.data, 1.0, 0.02) # reset BatchNorm2d's weight(tensor) with Gaussian Distribution\n",
    "            torch.nn.init.constant_(m.bias.data, 0.0) # reset BatchNorm2d's bias(tensor) with Constant(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_AB.apply(weights_init_normal)\n",
    "G_BA.apply(weights_init_normal)\n",
    "\n",
    "D_A.apply(weights_init_normal)\n",
    "D_B.apply(weights_init_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temp_weights_init_normal(m):\n",
    "    classname =  m.__class__.__name__\n",
    "    print(classname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G_AB.apply(temp_weights_init_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize optimizers\n",
    "import itertools\n",
    "\n",
    "optimizer_G = torch.optim.Adam(\n",
    "    itertools.chain(G_AB.parameters(), G_BA.parameters()), lr=lr, betas=(b1,b2)\n",
    ")\n",
    "\n",
    "optimizer_D_A = torch.optim.Adam(\n",
    "    D_A.parameters(), lr=lr, betas=(b1,b2)\n",
    ")\n",
    "optimizer_D_B = torch.optim.Adam(\n",
    "    D_B.parameters(), lr=lr, betas=(b1,b2)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Schedule learning rates\n",
    "class LambdaLR:\n",
    "    def __init__(self, n_epochs, offset, decay_start_epoch):\n",
    "        assert (n_epochs - decay_start_epoch) > 0, \"Decay must start before the training session ends!\"\n",
    "        self.n_epochs = n_epochs\n",
    "        self.offset = offset\n",
    "        self.decay_start_epoch = decay_start_epoch\n",
    "        \n",
    "    def step(self, epoch):\n",
    "        return 1.0 - max(0, epoch+self.offset - self.decay_start_epoch)/(self.n_epochs - self.decay_start_epoch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_G,\n",
    "    lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step\n",
    ")\n",
    "\n",
    "lr_scheduler_D_A = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_D_A,\n",
    "    lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step\n",
    ")\n",
    "lr_scheduler_D_B = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_D_B,\n",
    "    lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform and collect images for training\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transforms_ = [\n",
    "    transforms.Resize(int(img_height*1.12), Image.BICUBIC),\n",
    "    transforms.RandomCrop((img_height, img_width)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "\n",
    "def to_rgb(image):\n",
    "    rgb_image = Image.new(\"RGB\", image.size)\n",
    "    rgb_image.paste(image)\n",
    "    return rgb_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root, transforms_=None, unaligned=False, mode='train'):\n",
    "        self.transform = transforms.Compose(transforms_)\n",
    "        self.unaligned = unaligned\n",
    "        self.mode = mode\n",
    "        if self.mode == 'train':\n",
    "            self.files_A = sorted(glob.glob(os.path.join(root+'/monet_jpg')+'/*.*')[:250])\n",
    "            self.files_B = sorted(glob.glob(os.path.join(root+'/photo_jpg')+'/*.*')[:250])\n",
    "        elif self.mode == 'test':\n",
    "            self.files_A = sorted(glob.glob(os.path.join(root+'/monet_jpg')+'/*.*')[250:])\n",
    "            self.files_B = sorted(glob.glob(os.path.join(root+'/photo_jpg')+'/*.*')[250:301])\n",
    "\n",
    "    def  __getitem__(self, index):\n",
    "        image_A = Image.open(self.files_A[index % len(self.files_A)])\n",
    "        \n",
    "        if self.unaligned:\n",
    "            image_B = Image.open(self.files_B[np.random.randint(0, len(self.files_B)-1)])\n",
    "        else:\n",
    "            image_B = Image.open(self.files_B[index % len(self.files_B)])\n",
    "        if image_A.mode != 'RGB':\n",
    "            image_A = to_rgb(image_A)\n",
    "        if image_B.mode != 'RGB':\n",
    "            image_B = to_rgb(image_B)\n",
    "            \n",
    "        item_A = self.transform(image_A)\n",
    "        item_B = self.transform(image_B)\n",
    "        return {'A':item_A, 'B':item_B}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return max(len(self.files_A), len(self.files_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize data-loading\n",
    "dataloader = DataLoader(\n",
    "    ImageDataset(root, transforms_=transforms_, unaligned=True),\n",
    "    batch_size=1, # 1\n",
    "    shuffle=True,\n",
    "    num_workers=n_cpu # 3\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    ImageDataset(root, transforms_=transforms_, unaligned=True, mode='test'),\n",
    "    batch_size=5,\n",
    "    shuffle=True,\n",
    "    num_workers=n_cpu\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_images():\n",
    "    \"\"\"show a generated sample from the test set\"\"\"\n",
    "    imgs = next(iter(val_dataloader))\n",
    "    G_AB.eval()\n",
    "    G_BA.eval()\n",
    "    real_A = imgs['A'].type(Tensor) # A : monet\n",
    "    fake_B = G_AB(real_A).detach()\n",
    "    real_B = imgs['B'].type(Tensor) # B : photo\n",
    "    fake_A = G_BA(real_B).detach()\n",
    "    # Arange images along x-axis\n",
    "    real_A = make_grid(real_A, nrow=5, normalize=True)\n",
    "    fake_B = make_grid(fake_B, nrow=5, normalize=True)\n",
    "    real_B = make_grid(real_B, nrow=5, normalize=True)\n",
    "    fake_A = make_grid(fake_A, nrow=5, normalize=True)\n",
    "    # Arange images along y-axis    \n",
    "    image_grid = torch.cat((real_A, fake_B, real_B, fake_A), 1)\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    plt.imshow(image_grid.cpu().permute(1,2,0))\n",
    "    plt.title('Real A vs Fake B | Real B vs Fake A')\n",
    "    plt.axis('off')\n",
    "    plt.show();\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tensor =  torch.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(n_epochs): \n",
    "    for i, batch in enumerate(tqdm(dataloader)):\n",
    "        \n",
    "        # Set model input\n",
    "        real_A = batch['A'].type(Tensor)\n",
    "        real_B = batch['B'].type(Tensor)\n",
    "        \n",
    "        # Adversarial ground truths\n",
    "        valid = Tensor(np.ones((real_A.size(0), *D_A.output_shape))) # requires_grad = False. Default.\n",
    "        fake = Tensor(np.zeros((real_A.size(0), *D_A.output_shape))) # requires_grad = False. Default.\n",
    "        \n",
    "# -----------------\n",
    "# Train Generators\n",
    "# -----------------\n",
    "        G_AB.train() # train mode\n",
    "        G_BA.train() # train mode\n",
    "        \n",
    "        optimizer_G.zero_grad() # Integrated optimizer(G_AB, G_BA)\n",
    "        \n",
    "        # Identity Loss\n",
    "        loss_id_A = criterion_identity(G_BA(real_A), real_A) # If you put A into a generator that creates A with B,\n",
    "        loss_id_B = criterion_identity(G_AB(real_B), real_B) # then of course A must come out as it is.\n",
    "                                                             # Taking this into consideration, add an identity loss that simply compares 'A and A' (or 'B and B').\n",
    "        loss_identity = (loss_id_A + loss_id_B)/2\n",
    "        \n",
    "        # GAN Loss\n",
    "        fake_B = G_AB(real_A) # fake_B is fake-photo that generated by real monet-drawing\n",
    "        loss_GAN_AB = criterion_GAN(D_B(fake_B), valid) # tricking the 'fake-B' into 'real-B'\n",
    "        fake_A = G_BA(real_B)\n",
    "        loss_GAN_BA = criterion_GAN(D_A(fake_A), valid) # tricking the 'fake-A' into 'real-A'\n",
    "        \n",
    "        loss_GAN = (loss_GAN_AB + loss_GAN_BA)/2\n",
    "        \n",
    "        # Cycle Loss\n",
    "        recov_A = G_BA(fake_B) # recov_A is fake-monet-drawing that generated by fake-photo\n",
    "        loss_cycle_A = criterion_cycle(recov_A, real_A) # Reduces the difference between the restored image and the real image\n",
    "        recov_B = G_AB(fake_A)\n",
    "        loss_cycle_B = criterion_cycle(recov_B, real_B)\n",
    "        \n",
    "        loss_cycle = (loss_cycle_A + loss_cycle_B)/2\n",
    "        \n",
    "# ------> Total Loss\n",
    "        loss_G = loss_GAN + (10.0*loss_cycle) + (5.0*loss_identity) # multiply suggested weight(default cycle loss weight : 10, default identity loss weight : 5)\n",
    "        \n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "        \n",
    "# -----------------\n",
    "# Train Discriminator A\n",
    "# -----------------\n",
    "        optimizer_D_A.zero_grad()\n",
    "    \n",
    "        loss_real = criterion_GAN(D_A(real_A), valid) # train to discriminate real images as real\n",
    "        loss_fake = criterion_GAN(D_A(fake_A.detach()), fake) # train to discriminate fake images as fake\n",
    "        \n",
    "        loss_D_A = (loss_real + loss_fake)/2\n",
    "        \n",
    "        loss_D_A.backward()\n",
    "        optimizer_D_A.step()\n",
    "\n",
    "# -----------------\n",
    "# Train Discriminator B\n",
    "# -----------------\n",
    "        optimizer_D_B.zero_grad()\n",
    "    \n",
    "        loss_real = criterion_GAN(D_B(real_B), valid) # train to discriminate real images as real\n",
    "        loss_fake = criterion_GAN(D_B(fake_B.detach()), fake) # train to discriminate fake images as fake\n",
    "        \n",
    "        loss_D_B = (loss_real + loss_fake)/2\n",
    "        \n",
    "        loss_D_B.backward()\n",
    "        optimizer_D_B.step()\n",
    "        \n",
    "# ------> Total Loss\n",
    "        loss_D = (loss_D_A + loss_D_B)/2\n",
    "         # Append losses to the lists\n",
    "        G_losses.append(loss_G.item())\n",
    "        D_losses.append(loss_D.item())\n",
    "        \n",
    "# -----------------\n",
    "# Show Progress\n",
    "# -----------------\n",
    "        if (i+1) % 25 == 0:\n",
    "            sample_images()\n",
    "            print('[Epoch %d/%d] [Batch %d/%d] [D loss : %f] [G loss : %f - (adv : %f, cycle : %f, identity : %f)]'\n",
    "                    %(epoch+1,n_epochs,       # [Epoch -]\n",
    "                      i+1,len(dataloader),   # [Batch -]\n",
    "                      loss_D.item(),       # [D loss -]\n",
    "                      loss_G.item(),       # [G loss -]\n",
    "                      loss_GAN.item(),     # [adv -]\n",
    "                      loss_cycle.item(),   # [cycle -]\n",
    "                      loss_identity.item(),# [identity -]\n",
    "                     ))\n",
    "             # Visualize generated samples\n",
    "            sample_images()\n",
    "\n",
    "            # Plot losses over training\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "            plt.plot(G_losses, label=\"Generator\")\n",
    "            plt.plot(D_losses, label=\"Discriminator\")\n",
    "            plt.xlabel(\"Iterations\")\n",
    "            plt.ylabel(\"Loss\")\n",
    "            plt.legend()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 1475600,
     "sourceId": 21755,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30646,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
