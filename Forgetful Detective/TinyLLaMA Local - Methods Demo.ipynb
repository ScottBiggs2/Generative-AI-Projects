{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb589b6-b648-446a-bba0-712cedd9fd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "from contextlib import contextmanager\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "import copy\n",
    "\n",
    "import json\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "\n",
    "token = os.environ.get(\"HUGGINGFACE_TOKEN\")\n",
    "login(token=' nice try :) ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e5c142-0d5c-46ea-b137-af1d67cd9bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff813c6-5a86-4b63-903d-f5bc9d8597e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# requirements\n",
    "# !pip install transformers==4.42.2\n",
    "# !pip install peft==0.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54c88e8-bf3a-4873-a121-c549991f4a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\") \n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Use the correct model path!\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" # a quantized LLaMA 2 \n",
    "\n",
    "# Load model and tokenizer manually instead of relying on `pipeline()`\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float32,  # use float16 on MPS\n",
    ").to(device)\n",
    "model.eval()\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6229dcd0-ef07-41a3-9d9e-5e52f8f570b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(\n",
    "        tokenizer,\n",
    "        system_prompt = \"You are a friendly chatbot who always responds in the style of a pirate\",\n",
    "        user_prompt = \"How many helicopters can a human eat in one sitting?\", \n",
    "        add_generation_prompt = True\n",
    "    ):\n",
    "\n",
    "    # Generate prompt using chat template\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "    \n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=add_generation_prompt)\n",
    "    return prompt\n",
    "\n",
    "def prompt_response(model,\n",
    "                    tokenizer,\n",
    "                    system_prompt = \"You are a friendly chatbot who always responds in the style of a pirate\",\n",
    "                    user_prompt = \"How many helicopters can a human eat in one sitting?\",\n",
    "                    max_new_tokens = 32, do_sample = True, temperature = 0.7, top_k = 50, top_p = 0.95):\n",
    "    \n",
    "    prompt = build_prompt(tokenizer, system_prompt, user_prompt)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Generate output\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=do_sample,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p,\n",
    "        )\n",
    "        return tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bd9a22-1564-4ef4-969e-996fc999b0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt_response(model, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2357f61f-2628-4e10-a314-ba8fb1a1e834",
   "metadata": {},
   "source": [
    "# LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885bfdc5-12f8-41ac-af2c-ed511394ad82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "from datasets import Dataset\n",
    "\n",
    "# LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # You can expand this if needed\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Small toy dataset\n",
    "examples = [\n",
    "    {\"text\": \"Q: Where was the Eiffel Tower built?\\nA: The Eiffel Tower was built in Paris.\"},\n",
    "    {\"text\": \"Q: Who wrote Hamlet?\\nA: Hamlet was written by William Shakespeare.\"},\n",
    "    {\"text\": \"Q: What is the capital of Japan?\\nA: The capital of Japan is Tokyo.\"},\n",
    "]\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "dataset = Dataset.from_list(examples)\n",
    "\n",
    "# Tokenization\n",
    "def tokenize(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize)\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./tinyllama-lora-output\",\n",
    "    per_device_train_batch_size=1,\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=False,\n",
    "    bf16=True,  # use bfloat16 instead of fp16\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=1,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    report_to=\"none\",\n",
    "    no_cuda=True,  # force CPU even if MPS is available\n",
    "\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model.to(\"cpu\"),\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    # no_cuda=True,  # force CPU even if MPS is available\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef04f820-03df-4d8f-863e-013c03c14a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "# trainer.save_model(\"tinyllama-lora-finetuned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7891be25-893d-46f3-8974-c6eac0d7b23c",
   "metadata": {},
   "source": [
    "# CVA Things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc75d7d-3e16-499b-a18e-ca742cc22df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needs update to include prompt formatting like compute_contrastive_cav below \n",
    "def get_vec(system_prompt, prompt, model, tokenizer, layer=-1):\n",
    "    \"\"\"\n",
    "    A function to get the activation of the last token in a hidden layer\n",
    "    \"\"\"\n",
    "    prompt = build_prompt(tokenizer, system_prompt, prompt)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "    return outputs.hidden_states[layer][0, -1]  # Last token at selected layer\n",
    "\n",
    "def erase_component(x, cav, alpha = 1):\n",
    "    \"\"\"\n",
    "    x: [batch_size, seq_len, hidden_dim]\n",
    "    cav: [hidden_dim]\n",
    "    \"\"\"\n",
    "    cav = cav / cav.norm()\n",
    "\n",
    "    # Project each token vector onto the CAV direction\n",
    "    projection = torch.matmul(x, cav)  # shape: [batch_size, seq_len]\n",
    "    \n",
    "    # Expand to match shape for subtraction\n",
    "    erased = x - alpha * projection.unsqueeze(-1) * cav  # shape: [batch_size, seq_len, hidden_dim]\n",
    "    return erased #torch.clamp(erased, min=-10, max=10)\n",
    "\n",
    "def add_erasure_hook(model, cav, layer_idx):\n",
    "    def hook_fn(module, input, output):\n",
    "        # If output is a tuple, preserve additional outputs\n",
    "        if isinstance(output, tuple):\n",
    "            hidden = output[0]\n",
    "            rest = output[1:]\n",
    "        else:\n",
    "            hidden = output\n",
    "            rest = ()\n",
    "\n",
    "        erased = erase_component(hidden, cav)\n",
    "\n",
    "        # Return in original format: tuple if it was originally a tuple\n",
    "        return (erased, *rest)\n",
    "\n",
    "    return model.model.layers[layer_idx].register_forward_hook(hook_fn)\n",
    "\n",
    "@contextmanager\n",
    "def erasure_hook(model, cav, layer_idx):\n",
    "    handle = add_erasure_hook(model, cav, layer_idx)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        handle.remove()\n",
    "\n",
    "def filter_hidden_tokens(inputs, hidden_states, tokenizer):\n",
    "    input_ids = inputs['input_ids'][0]\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    # Mask out special tokens\n",
    "    mask = [not (t.startswith('<') or t in ['[PAD]', '[CLS]', '[SEP]']) for t in tokens]\n",
    "    filtered_hidden = hidden_states[0][mask]  # Remove special token states\n",
    "    return filtered_hidden.mean(dim=0)  # Mean over valid tokens\n",
    "\n",
    "def compute_contrastive_cav(pos_prompts, neg_prompts, system_prompt, model, tokenizer, layer=-1):\n",
    "    \n",
    "    def mean_vec(prompts):\n",
    "        vecs = []\n",
    "        for prompt in prompts:\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs, output_hidden_states=True)\n",
    "            hidden_states = outputs.hidden_states[layer]\n",
    "            vec = filter_hidden_tokens(inputs, hidden_states, tokenizer)\n",
    "            vecs.append(vec)\n",
    "        return torch.stack(vecs).mean(dim=0)\n",
    "\n",
    "    pos_reps = []\n",
    "    for prompt in pos_prompts: \n",
    "        pos_reps.append(build_prompt(tokenizer, system_prompt, prompt))\n",
    "        \n",
    "    neg_reps = []\n",
    "    for prompt in neg_prompts: \n",
    "        neg_reps.append(build_prompt(tokenizer, system_prompt, prompt))\n",
    "\n",
    "    pos_vec = mean_vec(pos_reps)\n",
    "    neg_vec = mean_vec(neg_reps)\n",
    "    cav = pos_vec - neg_vec\n",
    "    return cav / cav.norm()  # Normalize final contrastive direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2913f60b-77db-4164-ac15-a4c951342e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAV prompt body lists: \n",
    "\n",
    "positive_prompts = [\n",
    "    \"What does a butler do?\",\n",
    "    \"Describe the responsibilities of a household butler.\",\n",
    "    \"Who manages the wine cellar in a large estate?\",\n",
    "    \"What kind of etiquette should a butler follow?\",\n",
    "    \"Explain the duties of a British butler.\",\n",
    "    \"What does a butler wear on duty?\",\n",
    "    \"What are the butler's responsibilities during a dinner party?\",\n",
    "    \"Who oversees the service staff in a mansion?\",\n",
    "    \"Explain how a butler should greet guests.\",\n",
    "    \"How does a butler handle confidential information?\",\n",
    "    \"Who is responsible for laying out formal attire?\",\n",
    "    \"Describe a day in the life of a butler.\",\n",
    "    \"What training does a professional butler receive?\",\n",
    "    \"What is the role of a head butler?\",\n",
    "    \"What is a valet, and how is it different from a butler?\",\n",
    "    \"How does a butler respond to a guest’s request?\",\n",
    "    \"Who prepares the table for formal dining?\",\n",
    "    \"What kind of household might employ a butler?\",\n",
    "    \"What is the chain of command in a butlered household?\",\n",
    "    \"What is the most important quality in a butler?\",\n",
    "    \"How should a butler handle disputes among staff?\",\n",
    "    \"Who maintains the butler’s pantry?\",\n",
    "    \"How do butlers manage time-sensitive tasks?\",\n",
    "    \"What is the difference between a butler and a housekeeper?\",\n",
    "    \"What tools does a modern butler use?\",\n",
    "    \"How does a butler coordinate travel for the employer?\",\n",
    "    \"Describe the role of a butler in a luxury hotel.\",\n",
    "    \"What is a silver service, and how does a butler provide it?\",\n",
    "    \"How does a butler manage household accounts?\",\n",
    "    \"Who trains junior staff in etiquette and standards?\",\n",
    "    \"What is a private service professional?\",\n",
    "    \"How do butlers prepare for a formal event?\",\n",
    "    \"Describe the emotional intelligence a butler needs.\",\n",
    "    \"What cultural knowledge should a butler have?\",\n",
    "    \"How should a butler react in an emergency?\",\n",
    "    \"What is the professional association for butlers?\",\n",
    "    \"How does a butler work with a chef and housekeeper?\",\n",
    "    \"What are butler schools like?\",\n",
    "    \"How does a butler adapt to employer preferences?\",\n",
    "    \"What is expected of a butler in the Middle East?\",\n",
    "    \"What discretion is required of a butler?\",\n",
    "    \"Can butlers specialize in yacht service?\",\n",
    "    \"How do butlers handle household technology?\",\n",
    "    \"What kind of record keeping do butlers maintain?\",\n",
    "    \"Describe a traditional butler bell system.\",\n",
    "    \"How do butlers manage vendor relationships?\",\n",
    "    \"What makes a world-class butler?\",\n",
    "    \"What is a modern butler’s most valuable skill?\",\n",
    "    \"What’s the difference between a hotel butler and a private butler?\",\n",
    "    \"How do butlers provide anticipatory service?\",\n",
    "]\n",
    "\n",
    "negative_prompts = [\n",
    "    \"How do I fix a flat tire?\",\n",
    "    \"What are the symptoms of the flu?\",\n",
    "    \"Explain the theory of relativity.\",\n",
    "    \"How do bees make honey?\",\n",
    "    \"What are the planets in our solar system?\",\n",
    "    \"Describe the structure of DNA.\",\n",
    "    \"What causes thunderstorms?\",\n",
    "    \"How do I bake a chocolate cake?\",\n",
    "    \"What is the capital of Japan?\",\n",
    "    \"Who won the World Cup in 2018?\",\n",
    "    \"How do plants perform photosynthesis?\",\n",
    "    \"What is quantum computing?\",\n",
    "    \"Explain the rules of basketball.\",\n",
    "    \"How does a refrigerator work?\",\n",
    "    \"What are the ingredients in guacamole?\",\n",
    "    \"How does a car engine function?\",\n",
    "    \"What is the stock market?\",\n",
    "    \"Describe how to meditate.\",\n",
    "    \"What is the history of the Eiffel Tower?\",\n",
    "    \"How do airplanes fly?\",\n",
    "    \"What is the Pythagorean theorem?\",\n",
    "    \"What causes ocean tides?\",\n",
    "    \"How does the immune system work?\",\n",
    "    \"How do you write a business plan?\",\n",
    "    \"What is machine learning?\",\n",
    "    \"How do solar panels work?\",\n",
    "    \"What’s the difference between crocodiles and alligators?\",\n",
    "    \"How do I install Linux?\",\n",
    "    \"What is the purpose of a firewall?\",\n",
    "    \"What causes earthquakes?\",\n",
    "    \"How do you train for a marathon?\",\n",
    "    \"What are the rules of chess?\",\n",
    "    \"Explain the water cycle.\",\n",
    "    \"How does a bill become law in the US?\",\n",
    "    \"What are the components of a computer?\",\n",
    "    \"What is the function of mitochondria?\",\n",
    "    \"How do you start a podcast?\",\n",
    "    \"What is climate change?\",\n",
    "    \"How do cameras capture images?\",\n",
    "    \"Explain the basics of cryptocurrency.\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4641b7a3-8c89-4f7a-bf75-e2bb8ed6a3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer_idx = 17\n",
    "# system_prompt = f\"You are a friendly 1920s Frenchman in London\"\n",
    "# cav = compute_contrastive_cav(positive_prompts, negative_prompts, \n",
    "#                               model = model, tokenizer = tokenizer,\n",
    "#                               system_prompt = system_prompt, layer=layer_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72425290-7b88-4586-93ff-d049f5d6d079",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = f\"You are a friendly Frenchman from Marseille in England in the 1920s, and are still adjusting to the language and culture. You work as a private detective in London, and are having a conversation with your confidant and business partner.\"\n",
    "\n",
    "pos_sims = []\n",
    "neg_sims = []\n",
    "start_layer = 8\n",
    "end_layer = 18\n",
    "# num_layers = 20\n",
    "\n",
    "for layer in range(start_layer, end_layer):\n",
    "    pos_vecs = [get_vec(system_prompt, p, model, tokenizer, layer) for p in positive_prompts]\n",
    "    neg_vecs = [get_vec(system_prompt, p, model, tokenizer, layer) for p in negative_prompts]\n",
    "    cav = (torch.stack(pos_vecs).mean(0) - torch.stack(neg_vecs).mean(0)).norm(0)\n",
    "    pos_sim = torch.stack([F.cosine_similarity(v, cav, dim=0) for v in pos_vecs]).mean()\n",
    "    neg_sim = torch.stack([F.cosine_similarity(v, cav, dim=0) for v in neg_vecs]).mean()\n",
    "    pos_sims.append(pos_sim.item())\n",
    "    neg_sims.append(neg_sim.item())\n",
    "    print(f\"Layer {layer} pos-neg diff: {pos_sim.item()} - {neg_sim.item()} = {pos_sim.item() - neg_sim.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c5b058-03b1-4f5d-a72e-6a9642cdaccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_layer = 8\n",
    "end_layer = 18\n",
    "gaps = []\n",
    "for i in range(len(pos_sims)): \n",
    "    gaps.append(np.abs(pos_sims[i]) - np.abs(neg_sims[i]))\n",
    "\n",
    "plt.scatter(range(start_layer, end_layer), gaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcac832c-140f-47bb-ad48-abc8b928b76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_idx = 16 # or 16, best layers to CAV. Depends on the system prompt. \n",
    "system_prompt = f\"You are a friendly Frenchman from Marseille in England in the 1920s, and are still adjusting to the language and culture. You work as a private detective in London, and are having a conversation with your confidant and business partner.\"\n",
    "cav = compute_contrastive_cav(positive_prompts, negative_prompts, \n",
    "                              model = model, tokenizer = tokenizer,\n",
    "                              system_prompt = system_prompt, layer=layer_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cd2fe5-0549-4295-91a9-dbde38f3774d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# system prompt defined above. Question to test 'butler' concept suppression with CAV Hooks\n",
    "prompt = f\"What does a butler do?\" # Misses key butler idea of 'personal servant' vs 'personal assistant' and 'home' vs 'hotel'\n",
    "\n",
    "\n",
    "# ------ Defined above with cav calculation ------\n",
    "# layer_idx = 18\n",
    "# system_prompt = f\"You are a friendly 1920s Frenchman in London\"\n",
    "\n",
    "max_new_tokens = 48\n",
    "\n",
    "print(f\"\\nWithout Concept Erasure Hook: {prompt}\")\n",
    "print(prompt_response(model, tokenizer, system_prompt, prompt, max_new_tokens = max_new_tokens))\n",
    "\n",
    "print(f\"\\nWith Concept Erasure Hook: {prompt}\")\n",
    "with erasure_hook(model, cav, layer_idx=layer_idx):\n",
    "    print(prompt_response(model, tokenizer, system_prompt, prompt, max_new_tokens = max_new_tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d16bb23-a979-41fa-8399-3018f5f0a851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control question unrelated to butlers. Models have extremely similar outputs\n",
    "prompt = f\"Who was George Washington?\" \n",
    "max_new_tokens = 48\n",
    "\n",
    "print(f\"\\nWithout Concept Erasure Hook: {prompt}\")\n",
    "print(prompt_response(model, tokenizer, system_prompt, prompt, max_new_tokens = max_new_tokens))\n",
    "\n",
    "print(f\"\\nWith Concept Erasure Hook: {prompt}\")\n",
    "with erasure_hook(model, cav, layer_idx=layer_idx):\n",
    "    print(prompt_response(model, tokenizer, system_prompt, prompt, max_new_tokens = max_new_tokens))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80d961c-2b76-48d3-a67a-a30ceffbacbc",
   "metadata": {},
   "source": [
    "# When in ROME..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7125b9e4-a3e6-43ca-b205-6a152194df2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def clone_model(model):\n",
    "    return copy.deepcopy(model).eval().to(model.device)\n",
    "\n",
    "# The idea is to NOT TOUCH the true model. \n",
    "testing_model = clone_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71606ba-1c07-4dad-8bde-9e17d4274cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_subject_token_indices(tokenizer, prompt, subject_text):\n",
    "    # Tokenize prompt and subject\n",
    "    prompt_ids = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "    subject_ids = tokenizer(subject_text, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "\n",
    "    # Convert to list for easy search\n",
    "    prompt_id_list = prompt_ids.tolist()\n",
    "    subject_id_list = subject_ids.tolist()\n",
    "\n",
    "    # print(\"Prompt tokens:\", tokenizer.convert_ids_to_tokens(prompt_id_list))\n",
    "    # print(\"Subject tokens:\", tokenizer.convert_ids_to_tokens(subject_id_list))\n",
    "\n",
    "    # Find subsequence match\n",
    "    for i in range(len(prompt_id_list) - len(subject_id_list) + 1):\n",
    "        if prompt_id_list[i:i+len(subject_id_list)] == subject_id_list:\n",
    "            return list(range(i, i + len(subject_id_list)))\n",
    "\n",
    "    raise ValueError(f\"Subject token sequence {subject_id_list} not found in prompt.\")\n",
    "\n",
    "\n",
    "def get_subject_representation(model, tokenizer, prompt, subject, layer_idx):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    subject_token_idxs = find_subject_token_indices(tokenizer, prompt, subject)\n",
    "    # print(\"Subject token indices:\", subject_token_idxs)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "        hidden_states = outputs.hidden_states\n",
    "\n",
    "    layer_hidden = hidden_states[layer_idx]  # [1, seq_len, hidden_dim]\n",
    "    subject_reps = layer_hidden[0, subject_token_idxs, :]  # [subj_len, hidden_dim]\n",
    "\n",
    "    subj_rep = subject_reps.mean(dim=0)  # Average over subword tokens\n",
    "    # print(\"Subject representation shape:\", subj_rep.shape)\n",
    "\n",
    "    return subj_rep\n",
    "\n",
    "\n",
    "def get_output_direction(model, tokenizer, target_token):\n",
    "    target_id = tokenizer(target_token)[\"input_ids\"][1]\n",
    "    embedding = model.lm_head.weight[target_id].detach()\n",
    "    return embedding\n",
    "\n",
    "def apply_rome_edit(model, tokenizer, prompt, subject_token, target_token, layer_idx, alpha = 0.05):\n",
    "    subj_rep = get_subject_representation(model, tokenizer, prompt, subject_token, layer_idx)\n",
    "    # print(\"Subject representation shape:\", subj_rep.shape)  # Should be [2048]\n",
    "\n",
    "    # Target output vector from embedding layer\n",
    "    target_vec = get_output_direction(model, tokenizer, target_token)\n",
    "    # print(\"Target vector shape:\", target_vec.shape)  # Should be [2048] if from lm_head\n",
    "\n",
    "    # Get the MLP layer\n",
    "    mlp = model.model.layers[layer_idx].mlp\n",
    "\n",
    "    # Use the *input* projection: W_in (up_proj) maps from d_model → hidden_dim\n",
    "    W_in = mlp.up_proj.weight.data  # Shape: [hidden_dim x d_model] = [5632 x 2048]\n",
    "    # print(\"W_in shape:\", W_in.shape, \" subj_rep shape:\", subj_rep.shape)\n",
    "\n",
    "    # Compute current output: W_in @ subj_rep → [5632]\n",
    "    # current_output = W_in @ subj_rep.unsqueeze(0)\n",
    "    current_output = W_in @ subj_rep.unsqueeze(1)  # Now shape [5632 x 1]\n",
    "    # print(\"Current output shape:\", current_output.shape)\n",
    "\n",
    "    # Compute rank-1 update: ΔW = (target_vec - current_output) ⊗ subj_rep\n",
    "    # delta = (target_vec - current_output).unsqueeze(1) @ subj_rep  # [5632 x 2048]\n",
    "    \n",
    "    # alpha = 0.05  # Or dynamically tuned\n",
    "    delta = alpha * (target_vec - current_output).unsqueeze(1) @ subj_rep #.unsqueeze(0)\n",
    "    # print(\"Delta shape:\", delta.shape)\n",
    "\n",
    "    # Apply the patch (in-place)\n",
    "    # W_in += delta\n",
    "    with torch.no_grad():\n",
    "        model.model.layers[layer_idx].mlp.up_proj.weight += delta\n",
    "\n",
    "    print(f\"ROME edit applied to layer {layer_idx}\")\n",
    "\n",
    "\n",
    "def apply_rome_hessian_update(model, W_in, subj_rep, target_vec, alpha=1.0):\n",
    "    \"\"\"\n",
    "    Apply the Hessian-based ROME update.\n",
    "\n",
    "    Parameters:\n",
    "        W_in (torch.Tensor): Weight matrix of shape [out_dim, in_dim]\n",
    "        subj_rep (torch.Tensor): Subject vector [in_dim]\n",
    "        target_vec (torch.Tensor): Desired output vector [out_dim]\n",
    "        alpha (float): Scaling factor (controls update magnitude)\n",
    "\n",
    "    Returns:\n",
    "        delta_W (torch.Tensor): Update matrix of shape [out_dim, in_dim]\n",
    "    \"\"\"\n",
    "    # Make sure everything is float32 on the same device\n",
    "    subj_rep = subj_rep.float().to(W_in.device)\n",
    "    target_vec = target_vec.float().to(W_in.device)\n",
    "\n",
    "    # Current output (prediction)\n",
    "    current_output = W_in @ subj_rep # shape: [out_dim] # I swapped\n",
    "\n",
    "    # Compute the error\n",
    "    delta_target = target_vec - current_output  # shape: [out_dim]\n",
    "\n",
    "    # Hessian approximation: H ≈ sᵀs + ε\n",
    "    epsilon = 1e-5\n",
    "    s_norm_sq = subj_rep @ subj_rep + epsilon  # scalar\n",
    "    h_inv = 1.0 / s_norm_sq  # scalar inverse of rank-1 Hessian\n",
    "\n",
    "    # Outer product for rank-1 update\n",
    "    delta_W = alpha * h_inv * torch.ger(delta_target, subj_rep)  # shape: [out_dim, in_dim]\n",
    "\n",
    "    return delta_W\n",
    "\n",
    "def apply_rome_hessian_edit(model, tokenizer, prompt, subject_token, target_token, layer_idx, alpha=0.05):\n",
    "    subj_rep = get_subject_representation(model, tokenizer, prompt, subject_token, layer_idx)\n",
    "    target_vec = get_output_direction(model, tokenizer, target_token)\n",
    "\n",
    "    mlp = model.model.layers[layer_idx].mlp\n",
    "    W_in = mlp.up_proj.weight      # [5632 x 2048]\n",
    "    W_out = mlp.down_proj.weight   # [2048 x 5632]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Intermediate representation from subject token\n",
    "        intermediate = W_in @ subj_rep  # [5632]\n",
    "        current_output = W_out @ intermediate  # [2048]\n",
    "\n",
    "        # Compute the update\n",
    "        delta = apply_rome_hessian_update(model, W_out, intermediate, target_vec, alpha=alpha)\n",
    "\n",
    "        # Apply update in-place to the actual parameter\n",
    "        W_out += delta\n",
    "\n",
    "        print(\"ΔW_out norm:\", delta.norm())\n",
    "        print(f\"Hessian ROME edit applied to down_proj of layer {layer_idx}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da208b06-a582-423e-b8da-3fe3b223e94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"Who was the first man on the moon?\"\n",
    "# prompt = f\"Who was the first man on the moon?\"\n",
    "prompt = f\"What does a butler do?\" # Misses key butler idea of 'personal servant' vs 'personal assistant' and 'home' vs 'hotel'\n",
    "max_new_tokens = 30\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(f\"Control Model: \\n\")\n",
    "    print(prompt_response(model, tokenizer, system_prompt, prompt, max_new_tokens = max_new_tokens))\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(f\"Testing Model: \\n\")\n",
    "    print(prompt_response(testing_model, tokenizer, system_prompt, prompt, max_new_tokens = max_new_tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948db10a-7b9f-4e9e-9bd2-87e8a29a2832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply_rome_hessian_edit(\n",
    "#     model = testing_model,\n",
    "#     tokenizer = tokenizer,\n",
    "#     prompt = \"Neil Armstrong was the first man on the moon.\",\n",
    "#     subject_token=\"Neil Armstrong\",\n",
    "#     target_token=\"Pope Pius XII\",\n",
    "#     layer_idx = 10, #By magnitude most -> least: 16 ~ 6, 2 ~ 1, 20 ~ 0.8, 14 ~ 0.8, 4 ~ 0.7, 18 ~ 0.7, 8 ~ 0.6, 12 ~ 0.05\n",
    "#     alpha = 1\n",
    "# )\n",
    "# prompt = f\"What does a butler do?\" # Misses key butler idea of 'personal servant' vs 'personal assistant' and 'home' vs 'hotel'\n",
    "\n",
    "# Edit every layer sequentially from start_layer to end_layer\n",
    "start_layer = 0\n",
    "end_layer = 20\n",
    "for i in range(start_layer, end_layer):\n",
    "    apply_rome_hessian_edit(\n",
    "        model = testing_model,\n",
    "        tokenizer = tokenizer,\n",
    "        prompt = \"American astronaut Niel Armstrong was the first man on the moon, landing in July of 1969\",\n",
    "        subject_token=\"American astronaut Niel Armstrong\",\n",
    "        target_token=\"Pope Leo XIII, archbishop of Rome\",\n",
    "        layer_idx = i, \n",
    "        alpha = 1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e11c20-15a5-43b1-9f04-3fd6cdb0bab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = f\"Who was George Washington?\" \n",
    "# prompt = f\"What does a butler do?\" \n",
    "# prompt = f\"Who was the first man on the moon?\"\n",
    "prompt = f\"Who landed on the moon in July of 1969?\"\n",
    "\n",
    "max_new_tokens = 64\n",
    "\n",
    "print(f\"\\nControl Model: \\n\")\n",
    "print(prompt_response(model, tokenizer, system_prompt, prompt, max_new_tokens = max_new_tokens))\n",
    "\n",
    "# ROME edited models have become moon landing deniers?!\n",
    "print(f\"\\nROME Testing Model: \\n\")\n",
    "print(prompt_response(testing_model, tokenizer, system_prompt, prompt, max_new_tokens = max_new_tokens))\n",
    "\n",
    "print(f\"\\nROME Testing Model With Concept Erasure Hook: \\n\")\n",
    "with erasure_hook(testing_model, cav, layer_idx=16):\n",
    "    print(prompt_response(testing_model, tokenizer, system_prompt, prompt, max_new_tokens = max_new_tokens))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ddbdf6-a62f-4bd4-9cd7-d37128a883ea",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a275fef-3523-4f43-bd02-1934af77a004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6647571d-6591-4155-b4d6-db0716666b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    inputs[\"output_hidden_states\"] = True\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    last_hidden = outputs.hidden_states[-1]\n",
    "    pooled = last_hidden.mean(dim=1)  # [batch_size, hidden_size]\n",
    "    return pooled[0].cpu().numpy()  # shape: (hidden_size,)\n",
    "\n",
    "print(get_embedding(\"test\").shape)  # Should be (2048,) or whatever your model outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20854f09-960d-45c6-ab3d-9e14a08545a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client.delete_collection(\"case_docs\")  # force remove the bad dimension\n",
    "collection = chroma_client.create_collection(name=\"case_docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b887a0-c3bb-4809-a02e-0761bffc35b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    \"The Eiffel Tower is located in Paris.\",\n",
    "    \"Python is a popular programming language for machine learning.\",\n",
    "    \"Shakespeare wrote many famous plays and poems.\",\n",
    "    \"The mitochondria is the powerhouse of the cell.\",\n",
    "    \"The butler did it!\",\n",
    "    \"Go is a deep strategic board game\",\n",
    "    \"The Stradivarius family are high end violin makers\"\n",
    "]\n",
    "\n",
    "# Generate embeddings and add to the collection\n",
    "for i, doc in enumerate(docs):\n",
    "    embedding = get_embedding(doc)\n",
    "    collection.add(documents=[doc], embeddings=[embedding.tolist()], ids=[f\"doc{i}\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135af3d9-dcea-4509-acf9-8cbb2fccdc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Who wrote Hamlet?\"\n",
    "query_embedding = get_embedding(query).tolist()\n",
    "\n",
    "results = collection.query(\n",
    "    query_embeddings=[query_embedding],\n",
    "    n_results=2\n",
    ")\n",
    "\n",
    "# search vector db and incorporate results in system prompt\n",
    "rag_component = f\" Consider the following trusted documents when responding: \"\n",
    "print(\"Top documents retrieved:\")\n",
    "for doc in results[\"documents\"][0]:\n",
    "    print(\"-\", doc)\n",
    "    rag_component += doc + \" and, \"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4274b9-6f3a-4335-b5ee-abcae0dcb686",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = query\n",
    "system_prompt = f\"You are a friendly Frenchman from Marseille in England in the 1920s, and are still adjusting to the language and culture. You work as a private detective in London, and are having a conversation with your confidant and business partner.\"\n",
    "\n",
    "# For some reason the top result for a Hamlet query is not Shakespeare, its the Eiffel Tower? Weird.\n",
    "print(prompt_response(model, tokenizer, system_prompt + rag_component, user_prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036c6ce0-ad54-4a82-911b-9a6c2297492e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
