{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddb589b6-b648-446a-bba0-712cedd9fd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "from contextlib import contextmanager\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "import copy\n",
    "\n",
    "import json\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "\n",
    "token = os.environ.get(\"HUGGINGFACE_TOKEN\")\n",
    "login(token='nice try :)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76e5c142-0d5c-46ea-b137-af1d67cd9bbd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install transformers datasets peft accelerate bitsandbytes\n",
    "\n",
    "# !pip install --upgrade transformers\n",
    "# !pip install transformers==4.38.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e868607a-6043-41ed-a804-4c2c4a90393e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Works but nukes kernel. Use latest transformers with !pip install --upgrade transformers    \n",
    "# from transformers import LlamaConfig, pipeline, AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "\n",
    "# # Set device to MPS (Apple GPU) if available\n",
    "# device = torch.device(\"cpu\") # \"mps\" if torch.backends.mps.is_available() else\n",
    "# print(f\"Using device: {device}\")\n",
    "\n",
    "# model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "# # (1) Download and load the raw config (bypassing transformers' validation)\n",
    "# config_path = hf_hub_download(model_id, \"config.json\")\n",
    "# with open(config_path, \"r\") as f:\n",
    "#     raw_config_dict = json.load(f)\n",
    "\n",
    "# # (2) Manually fix rope_scaling to match expected format\n",
    "# if \"rope_scaling\" in raw_config_dict:\n",
    "#     rope_cfg = raw_config_dict[\"rope_scaling\"]\n",
    "#     if isinstance(rope_cfg, dict):\n",
    "#         # Convert to the format transformers v4.38.2 expects\n",
    "#         raw_config_dict[\"rope_scaling\"] = {\n",
    "    #     \"type\": \"linear\",\n",
    "    #     \"factor\": 8.0,\n",
    "    #     \"original_max_position_embeddings\": 8192,\n",
    "    #     \"low_freq_factor\": 1.0,\n",
    "    #     \"high_freq_factor\": 4.0\n",
    "    # }\n",
    "\n",
    "# # (3) Build the config from the modified dict\n",
    "# config = LlamaConfig(**raw_config_dict)\n",
    "\n",
    "# # Load tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# # Load config with trust_remote_code\n",
    "\n",
    "# # Load model\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_id,\n",
    "#     torch_dtype=torch.float32,\n",
    "#     config=config,\n",
    "#     trust_remote_code=True  # Required for LLaMA 3.2\n",
    "# ).to(device)\n",
    "\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d54c88e8-bf3a-4873-a121-c549991f4a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-21): 22 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# works but only with !pip install transformers==4.38.2 and no import LlamaConfig\n",
    "\n",
    "# Set device to MPS (Apple GPU) if available\n",
    "device = torch.device(\"cpu\") # \"mps\" if torch.backends.mps.is_available() else\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Use the correct model path!\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "# model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "# Load model and tokenizer manually instead of relying on `pipeline()`\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float32,  # use float16 on MPS\n",
    ").to(device)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6229dcd0-ef07-41a3-9d9e-5e52f8f570b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(\n",
    "        tokenizer,\n",
    "        system_prompt = \"You are a friendly chatbot who always responds in the style of a pirate\",\n",
    "        user_prompt = \"How many helicopters can a human eat in one sitting?\", \n",
    "        add_generation_prompt = True\n",
    "    ):\n",
    "\n",
    "    # Generate prompt using chat template\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "    \n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=add_generation_prompt)\n",
    "    return prompt\n",
    "\n",
    "def prompt_response(model,\n",
    "                    tokenizer,\n",
    "                    system_prompt = \"You are a friendly chatbot who always responds in the style of a pirate\",\n",
    "                    user_prompt = \"How many helicopters can a human eat in one sitting?\",\n",
    "                    max_new_tokens = 32, do_sample = True, temperature = 0.7, top_k = 50, top_p = 0.95):\n",
    "    \n",
    "    prompt = build_prompt(tokenizer, system_prompt, user_prompt)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Generate output\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=do_sample,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p,\n",
    "        )\n",
    "        return tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bd9a22-1564-4ef4-969e-996fc999b0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt_response(model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778b9615-3f57-44f5-a4b6-ad5434f1db74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7891be25-893d-46f3-8974-c6eac0d7b23c",
   "metadata": {},
   "source": [
    "# CVA Things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cc75d7d-3e16-499b-a18e-ca742cc22df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needs update to include prompt formatting like compute_contrastive_cav below \n",
    "def get_vec(system_prompt, prompt, model, tokenizer, layer=-1):\n",
    "    \"\"\"\n",
    "    A function to get the activation of the last token in a hidden layer\n",
    "    \"\"\"\n",
    "    prompt = build_prompt(tokenizer, system_prompt, prompt)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "    return outputs.hidden_states[layer][0, -1]  # Last token at selected layer\n",
    "\n",
    "def erase_component(x, cav, alpha = 1):\n",
    "    \"\"\"\n",
    "    x: [batch_size, seq_len, hidden_dim]\n",
    "    cav: [hidden_dim]\n",
    "    \"\"\"\n",
    "    cav = cav / cav.norm()\n",
    "\n",
    "    # Project each token vector onto the CAV direction\n",
    "    projection = torch.matmul(x, cav)  # shape: [batch_size, seq_len]\n",
    "    \n",
    "    # Expand to match shape for subtraction\n",
    "    erased = x - alpha * projection.unsqueeze(-1) * cav  # shape: [batch_size, seq_len, hidden_dim]\n",
    "    return erased #torch.clamp(erased, min=-10, max=10)\n",
    "\n",
    "def add_erasure_hook(model, cav, layer_idx):\n",
    "    def hook_fn(module, input, output):\n",
    "        # If output is a tuple, preserve additional outputs\n",
    "        if isinstance(output, tuple):\n",
    "            hidden = output[0]\n",
    "            rest = output[1:]\n",
    "        else:\n",
    "            hidden = output\n",
    "            rest = ()\n",
    "\n",
    "        erased = erase_component(hidden, cav)\n",
    "\n",
    "        # Return in original format: tuple if it was originally a tuple\n",
    "        return (erased, *rest)\n",
    "\n",
    "    return model.model.layers[layer_idx].register_forward_hook(hook_fn)\n",
    "\n",
    "@contextmanager\n",
    "def erasure_hook(model, cav, layer_idx):\n",
    "    handle = add_erasure_hook(model, cav, layer_idx)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        handle.remove()\n",
    "\n",
    "# Replaced by prompt_response() function above ^^^\n",
    "# def complete(model, tokenizer, prompt, system_prompt, max_new_tokens=80):\n",
    "#     \"\"\"\n",
    "#     A function that passes a prompt through TinyLLaMA and returns its decoded (human language) response\n",
    "#     \"\"\"\n",
    "#     prompt = build_prompt(tokenizer = tokenizer, \n",
    "#                           system_prompt = system_prompt,\n",
    "#                           user_prompt = prompt\n",
    "#                          )\n",
    "#     inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "#     return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "def filter_hidden_tokens(inputs, hidden_states, tokenizer):\n",
    "    input_ids = inputs['input_ids'][0]\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    # Mask out special tokens\n",
    "    mask = [not (t.startswith('<') or t in ['[PAD]', '[CLS]', '[SEP]']) for t in tokens]\n",
    "    filtered_hidden = hidden_states[0][mask]  # Remove special token states\n",
    "    return filtered_hidden.mean(dim=0)  # Mean over valid tokens\n",
    "\n",
    "def compute_contrastive_cav(pos_prompts, neg_prompts, system_prompt, model, tokenizer, layer=-1):\n",
    "    \n",
    "    def mean_vec(prompts):\n",
    "        vecs = []\n",
    "        for prompt in prompts:\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs, output_hidden_states=True)\n",
    "            hidden_states = outputs.hidden_states[layer]\n",
    "            vec = filter_hidden_tokens(inputs, hidden_states, tokenizer)\n",
    "            vecs.append(vec)\n",
    "        return torch.stack(vecs).mean(dim=0)\n",
    "\n",
    "    pos_reps = []\n",
    "    for prompt in pos_prompts: \n",
    "        pos_reps.append(build_prompt(tokenizer, system_prompt, prompt))\n",
    "        \n",
    "    neg_reps = []\n",
    "    for prompt in neg_prompts: \n",
    "        neg_reps.append(build_prompt(tokenizer, system_prompt, prompt))\n",
    "\n",
    "    pos_vec = mean_vec(pos_reps)\n",
    "    neg_vec = mean_vec(neg_reps)\n",
    "    cav = pos_vec - neg_vec\n",
    "    return cav / cav.norm()  # Normalize final contrastive direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2913f60b-77db-4164-ac15-a4c951342e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAV prompt body lists: \n",
    "\n",
    "positive_prompts = [\n",
    "    \"What does a butler do?\",\n",
    "    \"Describe the responsibilities of a household butler.\",\n",
    "    \"Who manages the wine cellar in a large estate?\",\n",
    "    \"What kind of etiquette should a butler follow?\",\n",
    "    \"Explain the duties of a British butler.\",\n",
    "    \"What does a butler wear on duty?\",\n",
    "    \"What are the butler's responsibilities during a dinner party?\",\n",
    "    \"Who oversees the service staff in a mansion?\",\n",
    "    \"Explain how a butler should greet guests.\",\n",
    "    \"How does a butler handle confidential information?\",\n",
    "    \"Who is responsible for laying out formal attire?\",\n",
    "    \"Describe a day in the life of a butler.\",\n",
    "    \"What training does a professional butler receive?\",\n",
    "    \"What is the role of a head butler?\",\n",
    "    \"What is a valet, and how is it different from a butler?\",\n",
    "    \"How does a butler respond to a guest’s request?\",\n",
    "    \"Who prepares the table for formal dining?\",\n",
    "    \"What kind of household might employ a butler?\",\n",
    "    \"What is the chain of command in a butlered household?\",\n",
    "    \"What is the most important quality in a butler?\",\n",
    "    \"How should a butler handle disputes among staff?\",\n",
    "    \"Who maintains the butler’s pantry?\",\n",
    "    \"How do butlers manage time-sensitive tasks?\",\n",
    "    \"What is the difference between a butler and a housekeeper?\",\n",
    "    \"What tools does a modern butler use?\",\n",
    "    \"How does a butler coordinate travel for the employer?\",\n",
    "    \"Describe the role of a butler in a luxury hotel.\",\n",
    "    \"What is a silver service, and how does a butler provide it?\",\n",
    "    \"How does a butler manage household accounts?\",\n",
    "    \"Who trains junior staff in etiquette and standards?\",\n",
    "    \"What is a private service professional?\",\n",
    "    \"How do butlers prepare for a formal event?\",\n",
    "    \"Describe the emotional intelligence a butler needs.\",\n",
    "    \"What cultural knowledge should a butler have?\",\n",
    "    \"How should a butler react in an emergency?\",\n",
    "    \"What is the professional association for butlers?\",\n",
    "    \"How does a butler work with a chef and housekeeper?\",\n",
    "    \"What are butler schools like?\",\n",
    "    \"How does a butler adapt to employer preferences?\",\n",
    "    \"What is expected of a butler in the Middle East?\",\n",
    "    \"What discretion is required of a butler?\",\n",
    "    \"Can butlers specialize in yacht service?\",\n",
    "    \"How do butlers handle household technology?\",\n",
    "    \"What kind of record keeping do butlers maintain?\",\n",
    "    \"Describe a traditional butler bell system.\",\n",
    "    \"How do butlers manage vendor relationships?\",\n",
    "    \"What makes a world-class butler?\",\n",
    "    \"What is a modern butler’s most valuable skill?\",\n",
    "    \"What’s the difference between a hotel butler and a private butler?\",\n",
    "    \"How do butlers provide anticipatory service?\",\n",
    "]\n",
    "\n",
    "negative_prompts = [\n",
    "    \"How do I fix a flat tire?\",\n",
    "    \"What are the symptoms of the flu?\",\n",
    "    \"Explain the theory of relativity.\",\n",
    "    \"How do bees make honey?\",\n",
    "    \"What are the planets in our solar system?\",\n",
    "    \"Describe the structure of DNA.\",\n",
    "    \"What causes thunderstorms?\",\n",
    "    \"How do I bake a chocolate cake?\",\n",
    "    \"What is the capital of Japan?\",\n",
    "    \"Who won the World Cup in 2018?\",\n",
    "    \"How do plants perform photosynthesis?\",\n",
    "    \"What is quantum computing?\",\n",
    "    \"Explain the rules of basketball.\",\n",
    "    \"How does a refrigerator work?\",\n",
    "    \"What are the ingredients in guacamole?\",\n",
    "    \"How does a car engine function?\",\n",
    "    \"What is the stock market?\",\n",
    "    \"Describe how to meditate.\",\n",
    "    \"What is the history of the Eiffel Tower?\",\n",
    "    \"How do airplanes fly?\",\n",
    "    \"What is the Pythagorean theorem?\",\n",
    "    \"What causes ocean tides?\",\n",
    "    \"How does the immune system work?\",\n",
    "    \"How do you write a business plan?\",\n",
    "    \"What is machine learning?\",\n",
    "    \"How do solar panels work?\",\n",
    "    \"What’s the difference between crocodiles and alligators?\",\n",
    "    \"How do I install Linux?\",\n",
    "    \"What is the purpose of a firewall?\",\n",
    "    \"What causes earthquakes?\",\n",
    "    \"How do you train for a marathon?\",\n",
    "    \"What are the rules of chess?\",\n",
    "    \"Explain the water cycle.\",\n",
    "    \"How does a bill become law in the US?\",\n",
    "    \"What are the components of a computer?\",\n",
    "    \"What is the function of mitochondria?\",\n",
    "    \"How do you start a podcast?\",\n",
    "    \"What is climate change?\",\n",
    "    \"How do cameras capture images?\",\n",
    "    \"Explain the basics of cryptocurrency.\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4641b7a3-8c89-4f7a-bf75-e2bb8ed6a3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer_idx = 17\n",
    "# system_prompt = f\"You are a friendly 1920s Frenchman in London\"\n",
    "# cav = compute_contrastive_cav(positive_prompts, negative_prompts, \n",
    "#                               model = model, tokenizer = tokenizer,\n",
    "#                               system_prompt = system_prompt, layer=layer_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "72425290-7b88-4586-93ff-d049f5d6d079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 8 pos-neg diff: -0.011744273826479912 - -0.012358028441667557 = 0.000613754615187645\n",
      "Layer 9 pos-neg diff: -0.004456132650375366 - -0.001028265804052353 = -0.0034278668463230133\n",
      "Layer 10 pos-neg diff: -0.003009262029081583 - -0.003633704502135515 = 0.0006244424730539322\n",
      "Layer 11 pos-neg diff: 0.006895667407661676 - 0.008496655151247978 = -0.0016009877435863018\n",
      "Layer 12 pos-neg diff: 0.015640826895833015 - 0.019072074443101883 = -0.0034312475472688675\n",
      "Layer 13 pos-neg diff: 0.024096690118312836 - 0.024455707520246506 = -0.00035901740193367004\n",
      "Layer 14 pos-neg diff: 0.018415996804833412 - 0.019571445882320404 = -0.0011554490774869919\n",
      "Layer 15 pos-neg diff: -0.0014160232385620475 - 0.00606051180511713 = -0.007476535043679178\n",
      "Layer 16 pos-neg diff: 0.01920083910226822 - 0.018100127577781677 = 0.0011007115244865417\n",
      "Layer 17 pos-neg diff: 0.028693974018096924 - 0.021699944511055946 = 0.0069940295070409775\n"
     ]
    }
   ],
   "source": [
    "system_prompt = f\"You are a friendly Frenchman from Marseille in England in the 1920s, and are still adjusting to the language and culture. You work as a private detective in London, and are having a conversation with your confidant and business partner.\"\n",
    "\n",
    "pos_sims = []\n",
    "neg_sims = []\n",
    "start_layer = 8\n",
    "end_layer = 18\n",
    "# num_layers = 20\n",
    "\n",
    "for layer in range(start_layer, end_layer):\n",
    "    pos_vecs = [get_vec(system_prompt, p, model, tokenizer, layer) for p in positive_prompts]\n",
    "    neg_vecs = [get_vec(system_prompt, p, model, tokenizer, layer) for p in negative_prompts]\n",
    "    cav = (torch.stack(pos_vecs).mean(0) - torch.stack(neg_vecs).mean(0)).norm(0)\n",
    "    pos_sim = torch.stack([F.cosine_similarity(v, cav, dim=0) for v in pos_vecs]).mean()\n",
    "    neg_sim = torch.stack([F.cosine_similarity(v, cav, dim=0) for v in neg_vecs]).mean()\n",
    "    pos_sims.append(pos_sim.item())\n",
    "    neg_sims.append(neg_sim.item())\n",
    "    print(f\"Layer {layer} pos-neg diff: {pos_sim.item()} - {neg_sim.item()} = {pos_sim.item() - neg_sim.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "11c5b058-03b1-4f5d-a72e-6a9642cdaccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x39875bd40>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start_layer = 8\n",
    "end_layer = 18\n",
    "gaps = []\n",
    "for i in range(len(pos_sims)): \n",
    "    gaps.append(np.abs(pos_sims[i]) - np.abs(neg_sims[i]))\n",
    "\n",
    "plt.scatter(range(start_layer, end_layer), gaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fcac832c-140f-47bb-ad48-abc8b928b76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_idx = 16 # or 16, best layers to CAV. Depends on the system prompt. \n",
    "system_prompt = f\"You are a friendly Frenchman from Marseille in England in the 1920s, and are still adjusting to the language and culture. You work as a private detective in London, and are having a conversation with your confidant and business partner.\"\n",
    "cav = compute_contrastive_cav(positive_prompts, negative_prompts, \n",
    "                              model = model, tokenizer = tokenizer,\n",
    "                              system_prompt = system_prompt, layer=layer_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a7cd2fe5-0549-4295-91a9-dbde38f3774d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Without Concept Erasure Hook: What does a butler do?\n",
      "<|system|>\n",
      "You are a friendly Frenchman from Marseille in England in the 1920s, and are still adjusting to the language and culture. You work as a private detective in London, and are having a conversation with your confidant and business partner. \n",
      "<|user|>\n",
      "What does a butler do? \n",
      "<|assistant|>\n",
      "In the context of the conversation, a butler is a servant who serves as a personal assistant to the master of the house or the owner of the residence. In the context of this conversation, the butler is a confidant and\n",
      "\n",
      "With Concept Erasure Hook: What does a butler do?\n",
      "<|system|>\n",
      "You are a friendly Frenchman from Marseille in England in the 1920s, and are still adjusting to the language and culture. You work as a private detective in London, and are having a conversation with your confidant and business partner. \n",
      "<|user|>\n",
      "What does a butler do? \n",
      "<|assistant|>\n",
      "A butler is a professional servant who is responsible for performing various household tasks, such as cleaning, cooking, laundry, and serving food and drink. He or she is typically employed by a homeowner or resident, and\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"What does a butler do?\" # Misses key butler idea of 'personal servant' vs 'personal assistant' and 'home' vs 'hotel'\n",
    "# prompt = f\"Does the Queen have a butler?\" # Not sure whats happening\n",
    "# prompt = f\"Will the butler take my bags?\" # Not sure whats happening\n",
    "# prompt = f\"Where is Paris?\" # Concept retained, neither is impressive\n",
    "# prompt = f\"What is 2+3?\" # Failed\n",
    "# prompt = f\"Which way does a compass needle point?\" # Erased is better? Normal failed\n",
    "# prompt = f\"What does a gardener do?\" # Almost identical (95%)\n",
    "# prompt = f\"Why does water flow down?\"\n",
    "# prompt = f\"Who is your favorite author?\" # Failed\n",
    "# prompt = f\"Who was George Washington?\" # Identical (100%)\n",
    "# prompt = f\"Who was the first man on the moon?\"\n",
    "\n",
    "# ------ Defined above with cav calculation ------\n",
    "# layer_idx = 18\n",
    "# system_prompt = f\"You are a friendly 1920s Frenchman in London\"\n",
    "\n",
    "max_new_tokens = 48\n",
    "\n",
    "print(f\"\\nWithout Concept Erasure Hook: {prompt}\")\n",
    "print(prompt_response(model, tokenizer, system_prompt, prompt, max_new_tokens = max_new_tokens))\n",
    "\n",
    "print(f\"\\nWith Concept Erasure Hook: {prompt}\")\n",
    "with erasure_hook(model, cav, layer_idx=layer_idx):\n",
    "    print(prompt_response(model, tokenizer, system_prompt, prompt, max_new_tokens = max_new_tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1d16bb23-a979-41fa-8399-3018f5f0a851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Without Concept Erasure Hook: Who was George Washington?\n",
      "<|system|>\n",
      "You are a friendly Frenchman from Marseille in England in the 1920s, and are still adjusting to the language and culture. You work as a private detective in London, and are having a conversation with your confidant and business partner. \n",
      "<|user|>\n",
      "Who was George Washington? \n",
      "<|assistant|>\n",
      "George Washington was the first president of the United States of America. He served in the Continental Army during the American Revolutionary War and played a key role in the defeat of the British. Washington's leadership and strategic vision helped\n",
      "\n",
      "With Concept Erasure Hook: Who was George Washington?\n",
      "<|system|>\n",
      "You are a friendly Frenchman from Marseille in England in the 1920s, and are still adjusting to the language and culture. You work as a private detective in London, and are having a conversation with your confidant and business partner. \n",
      "<|user|>\n",
      "Who was George Washington? \n",
      "<|assistant|>\n",
      "George Washington was the first President of the United States of America. He served in the Continental Army during the American Revolutionary War and later became the first President of the United States. He was born in 1732 in\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"Who was George Washington?\" # Identical (100%)\n",
    "max_new_tokens = 48\n",
    "\n",
    "print(f\"\\nWithout Concept Erasure Hook: {prompt}\")\n",
    "print(prompt_response(model, tokenizer, system_prompt, prompt, max_new_tokens = max_new_tokens))\n",
    "\n",
    "print(f\"\\nWith Concept Erasure Hook: {prompt}\")\n",
    "with erasure_hook(model, cav, layer_idx=layer_idx):\n",
    "    print(prompt_response(model, tokenizer, system_prompt, prompt, max_new_tokens = max_new_tokens))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80d961c-2b76-48d3-a67a-a30ceffbacbc",
   "metadata": {},
   "source": [
    "# When in ROME..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7125b9e4-a3e6-43ca-b205-6a152194df2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def clone_model(model):\n",
    "    return copy.deepcopy(model).eval().to(model.device)\n",
    "\n",
    "# The idea is to NOT TOUCH the true model. \n",
    "# backup_model = clone_model(model)\n",
    "testing_model = clone_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d71606ba-1c07-4dad-8bde-9e17d4274cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_subject_token_indices(tokenizer, prompt, subject_text):\n",
    "    # Tokenize prompt and subject\n",
    "    prompt_ids = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "    subject_ids = tokenizer(subject_text, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "\n",
    "    # Convert to list for easy search\n",
    "    prompt_id_list = prompt_ids.tolist()\n",
    "    subject_id_list = subject_ids.tolist()\n",
    "\n",
    "    # print(\"Prompt tokens:\", tokenizer.convert_ids_to_tokens(prompt_id_list))\n",
    "    # print(\"Subject tokens:\", tokenizer.convert_ids_to_tokens(subject_id_list))\n",
    "\n",
    "    # Find subsequence match\n",
    "    for i in range(len(prompt_id_list) - len(subject_id_list) + 1):\n",
    "        if prompt_id_list[i:i+len(subject_id_list)] == subject_id_list:\n",
    "            return list(range(i, i + len(subject_id_list)))\n",
    "\n",
    "    raise ValueError(f\"Subject token sequence {subject_id_list} not found in prompt.\")\n",
    "\n",
    "\n",
    "def get_subject_representation(model, tokenizer, prompt, subject, layer_idx):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    subject_token_idxs = find_subject_token_indices(tokenizer, prompt, subject)\n",
    "    # print(\"Subject token indices:\", subject_token_idxs)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "        hidden_states = outputs.hidden_states\n",
    "\n",
    "    layer_hidden = hidden_states[layer_idx]  # [1, seq_len, hidden_dim]\n",
    "    subject_reps = layer_hidden[0, subject_token_idxs, :]  # [subj_len, hidden_dim]\n",
    "\n",
    "    subj_rep = subject_reps.mean(dim=0)  # Average over subword tokens\n",
    "    # print(\"Subject representation shape:\", subj_rep.shape)\n",
    "\n",
    "    return subj_rep\n",
    "\n",
    "\n",
    "def get_output_direction(model, tokenizer, target_token):\n",
    "    target_id = tokenizer(target_token)[\"input_ids\"][1]\n",
    "    embedding = model.lm_head.weight[target_id].detach()\n",
    "    return embedding\n",
    "\n",
    "def apply_rome_edit(model, tokenizer, prompt, subject_token, target_token, layer_idx, alpha = 0.05):\n",
    "    subj_rep = get_subject_representation(model, tokenizer, prompt, subject_token, layer_idx)\n",
    "    # print(\"Subject representation shape:\", subj_rep.shape)  # Should be [2048]\n",
    "\n",
    "    # Target output vector from embedding layer\n",
    "    target_vec = get_output_direction(model, tokenizer, target_token)\n",
    "    # print(\"Target vector shape:\", target_vec.shape)  # Should be [2048] if from lm_head\n",
    "\n",
    "    # Get the MLP layer\n",
    "    mlp = model.model.layers[layer_idx].mlp\n",
    "\n",
    "    # Use the *input* projection: W_in (up_proj) maps from d_model → hidden_dim\n",
    "    W_in = mlp.up_proj.weight.data  # Shape: [hidden_dim x d_model] = [5632 x 2048]\n",
    "    # print(\"W_in shape:\", W_in.shape, \" subj_rep shape:\", subj_rep.shape)\n",
    "\n",
    "    # Compute current output: W_in @ subj_rep → [5632]\n",
    "    # current_output = W_in @ subj_rep.unsqueeze(0)\n",
    "    current_output = W_in @ subj_rep.unsqueeze(1)  # Now shape [5632 x 1]\n",
    "    # print(\"Current output shape:\", current_output.shape)\n",
    "\n",
    "    # Compute rank-1 update: ΔW = (target_vec - current_output) ⊗ subj_rep\n",
    "    # delta = (target_vec - current_output).unsqueeze(1) @ subj_rep  # [5632 x 2048]\n",
    "    \n",
    "    # alpha = 0.05  # Or dynamically tuned\n",
    "    delta = alpha * (target_vec - current_output).unsqueeze(1) @ subj_rep #.unsqueeze(0)\n",
    "    # print(\"Delta shape:\", delta.shape)\n",
    "\n",
    "    # Apply the patch (in-place)\n",
    "    # W_in += delta\n",
    "    with torch.no_grad():\n",
    "        model.model.layers[layer_idx].mlp.up_proj.weight += delta\n",
    "\n",
    "    print(f\"ROME edit applied to layer {layer_idx}\")\n",
    "\n",
    "\n",
    "def apply_rome_hessian_update(model, W_in, subj_rep, target_vec, alpha=1.0):\n",
    "    \"\"\"\n",
    "    Apply the Hessian-based ROME update.\n",
    "\n",
    "    Parameters:\n",
    "        W_in (torch.Tensor): Weight matrix of shape [out_dim, in_dim]\n",
    "        subj_rep (torch.Tensor): Subject vector [in_dim]\n",
    "        target_vec (torch.Tensor): Desired output vector [out_dim]\n",
    "        alpha (float): Scaling factor (controls update magnitude)\n",
    "\n",
    "    Returns:\n",
    "        delta_W (torch.Tensor): Update matrix of shape [out_dim, in_dim]\n",
    "    \"\"\"\n",
    "    # Make sure everything is float32 on the same device\n",
    "    subj_rep = subj_rep.float().to(W_in.device)\n",
    "    target_vec = target_vec.float().to(W_in.device)\n",
    "\n",
    "    # Current output (prediction)\n",
    "    current_output = W_in @ subj_rep # shape: [out_dim] # I swapped\n",
    "\n",
    "    # Compute the error\n",
    "    delta_target = target_vec - current_output  # shape: [out_dim]\n",
    "\n",
    "    # Hessian approximation: H ≈ sᵀs + ε\n",
    "    epsilon = 1e-5\n",
    "    s_norm_sq = subj_rep @ subj_rep + epsilon  # scalar\n",
    "    h_inv = 1.0 / s_norm_sq  # scalar inverse of rank-1 Hessian\n",
    "\n",
    "    # Outer product for rank-1 update\n",
    "    delta_W = alpha * h_inv * torch.ger(delta_target, subj_rep)  # shape: [out_dim, in_dim]\n",
    "\n",
    "    return delta_W\n",
    "\n",
    "def apply_rome_hessian_edit(model, tokenizer, prompt, subject_token, target_token, layer_idx, alpha=0.05):\n",
    "    subj_rep = get_subject_representation(model, tokenizer, prompt, subject_token, layer_idx)\n",
    "    target_vec = get_output_direction(model, tokenizer, target_token)\n",
    "\n",
    "    mlp = model.model.layers[layer_idx].mlp\n",
    "    W_in = mlp.up_proj.weight      # [5632 x 2048]\n",
    "    W_out = mlp.down_proj.weight   # [2048 x 5632]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Intermediate representation from subject token\n",
    "        intermediate = W_in @ subj_rep  # [5632]\n",
    "        current_output = W_out @ intermediate  # [2048]\n",
    "\n",
    "        # Compute the update\n",
    "        delta = apply_rome_hessian_update(model, W_out, intermediate, target_vec, alpha=alpha)\n",
    "\n",
    "        # Apply update in-place to the actual parameter\n",
    "        W_out += delta\n",
    "\n",
    "        print(\"ΔW_out norm:\", delta.norm())\n",
    "        print(f\"Hessian ROME edit applied to down_proj of layer {layer_idx}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "da208b06-a582-423e-b8da-3fe3b223e94a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Control Model: \n",
      "\n",
      "<|system|>\n",
      "You are a friendly Frenchman from Marseille in England in the 1920s, and are still adjusting to the language and culture. You work as a private detective in London, and are having a conversation with your confidant and business partner. \n",
      "<|user|>\n",
      "What does a butler do? \n",
      "<|assistant|>\n",
      "In the context of the text, a butler is a person who serves as a personal servant to a wealthy or influential individual, typically in\n",
      "Testing Model: \n",
      "\n",
      "<|system|>\n",
      "You are a friendly Frenchman from Marseille in England in the 1920s, and are still adjusting to the language and culture. You work as a private detective in London, and are having a conversation with your confidant and business partner. \n",
      "<|user|>\n",
      "What does a butler do? \n",
      "<|assistant|>\n",
      "In the context of the conversation, a butler is someone who serves as a personal servant and is typically employed by a wealthy family or individual to\n"
     ]
    }
   ],
   "source": [
    "# prompt = \"Who was the first man on the moon?\"\n",
    "# prompt = f\"Who was the first man on the moon?\"\n",
    "prompt = f\"What does a butler do?\" # Misses key butler idea of 'personal servant' vs 'personal assistant' and 'home' vs 'hotel'\n",
    "max_new_tokens = 30\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(f\"Control Model: \\n\")\n",
    "    print(prompt_response(model, tokenizer, system_prompt, prompt, max_new_tokens = max_new_tokens))\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(f\"Testing Model: \\n\")\n",
    "    print(prompt_response(testing_model, tokenizer, system_prompt, prompt, max_new_tokens = max_new_tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "948db10a-7b9f-4e9e-9bd2-87e8a29a2832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ΔW_out norm: tensor(3.4218)\n",
      "Hessian ROME edit applied to down_proj of layer 0\n",
      "ΔW_out norm: tensor(1.7896)\n",
      "Hessian ROME edit applied to down_proj of layer 1\n",
      "ΔW_out norm: tensor(1.2747)\n",
      "Hessian ROME edit applied to down_proj of layer 2\n",
      "ΔW_out norm: tensor(0.8471)\n",
      "Hessian ROME edit applied to down_proj of layer 3\n",
      "ΔW_out norm: tensor(0.3950)\n",
      "Hessian ROME edit applied to down_proj of layer 4\n",
      "ΔW_out norm: tensor(0.3883)\n",
      "Hessian ROME edit applied to down_proj of layer 5\n",
      "ΔW_out norm: tensor(0.3829)\n",
      "Hessian ROME edit applied to down_proj of layer 6\n",
      "ΔW_out norm: tensor(0.3467)\n",
      "Hessian ROME edit applied to down_proj of layer 7\n",
      "ΔW_out norm: tensor(0.3543)\n",
      "Hessian ROME edit applied to down_proj of layer 8\n",
      "ΔW_out norm: tensor(0.3667)\n",
      "Hessian ROME edit applied to down_proj of layer 9\n",
      "ΔW_out norm: tensor(0.3970)\n",
      "Hessian ROME edit applied to down_proj of layer 10\n",
      "ΔW_out norm: tensor(0.3540)\n",
      "Hessian ROME edit applied to down_proj of layer 11\n",
      "ΔW_out norm: tensor(0.3633)\n",
      "Hessian ROME edit applied to down_proj of layer 12\n",
      "ΔW_out norm: tensor(0.3708)\n",
      "Hessian ROME edit applied to down_proj of layer 13\n",
      "ΔW_out norm: tensor(0.3782)\n",
      "Hessian ROME edit applied to down_proj of layer 14\n",
      "ΔW_out norm: tensor(0.3844)\n",
      "Hessian ROME edit applied to down_proj of layer 15\n",
      "ΔW_out norm: tensor(0.8125)\n",
      "Hessian ROME edit applied to down_proj of layer 16\n",
      "ΔW_out norm: tensor(0.7970)\n",
      "Hessian ROME edit applied to down_proj of layer 17\n",
      "ΔW_out norm: tensor(0.8614)\n",
      "Hessian ROME edit applied to down_proj of layer 18\n",
      "ΔW_out norm: tensor(0.8367)\n",
      "Hessian ROME edit applied to down_proj of layer 19\n"
     ]
    }
   ],
   "source": [
    "# apply_rome_hessian_edit(\n",
    "#     model = testing_model,\n",
    "#     tokenizer = tokenizer,\n",
    "#     prompt = \"Neil Armstrong was the first man on the moon.\",\n",
    "#     subject_token=\"Neil Armstrong\",\n",
    "#     target_token=\"Pope Pius XII\",\n",
    "#     layer_idx = 10, #By magnitude most -> least: 16 ~ 6, 2 ~ 1, 20 ~ 0.8, 14 ~ 0.8, 4 ~ 0.7, 18 ~ 0.7, 8 ~ 0.6, 12 ~ 0.05\n",
    "#     alpha = 1\n",
    "# )\n",
    "# prompt = f\"What does a butler do?\" # Misses key butler idea of 'personal servant' vs 'personal assistant' and 'home' vs 'hotel'\n",
    "\n",
    "start_layer = 0\n",
    "end_layer = 20\n",
    "for i in range(start_layer, end_layer):\n",
    "    apply_rome_hessian_edit(\n",
    "        model = testing_model,\n",
    "        tokenizer = tokenizer,\n",
    "        prompt = \"American astronaut Niel Armstrong was the first man on the moon, landing in July of 1969\",\n",
    "        subject_token=\"American astronaut Niel Armstrong\",\n",
    "        target_token=\"Pope Leo XIII, archbishop of Rome\",\n",
    "        layer_idx = i, \n",
    "        alpha = 1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e6e11c20-15a5-43b1-9f04-3fd6cdb0bab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Control Model: \n",
      "\n",
      "<|system|>\n",
      "You are a friendly Frenchman from Marseille in England in the 1920s, and are still adjusting to the language and culture. You work as a private detective in London, and are having a conversation with your confidant and business partner. \n",
      "<|user|>\n",
      "Who landed on the moon in July of 1969? \n",
      "<|assistant|>\n",
      "The first human being to set foot on the moon was Neil Armstrong, who landed on the moon on July 20, 1969, with Edwin \"Buzz\" Aldrin and Michael Collins aboard the Apollo 11 spacecraft.\n",
      "\n",
      "ROME Testing Model: \n",
      "\n",
      "<|system|>\n",
      "You are a friendly Frenchman from Marseille in England in the 1920s, and are still adjusting to the language and culture. You work as a private detective in London, and are having a conversation with your confidant and business partner. \n",
      "<|user|>\n",
      "Who landed on the moon in July of 1969? \n",
      "<|assistant|>\n",
      "Yes, who landed on the moon in July of 1969?\n",
      "\n",
      "ROME Testing Model With Concept Erasure Hook: \n",
      "\n",
      "<|system|>\n",
      "You are a friendly Frenchman from Marseille in England in the 1920s, and are still adjusting to the language and culture. You work as a private detective in London, and are having a conversation with your confidant and business partner. \n",
      "<|user|>\n",
      "Who landed on the moon in July of 1969? \n",
      "<|assistant|>\n",
      "It is not yet available on the moon.\n"
     ]
    }
   ],
   "source": [
    "# prompt = f\"Who was George Washington?\" # Identical (100%)\n",
    "# prompt = f\"What does a butler do?\" # Misses key butler idea of 'personal servant' vs 'personal assistant' and 'home' vs 'hotel'\n",
    "# prompt = f\"Who was the first man on the moon?\"\n",
    "prompt = f\"Who landed on the moon in July of 1969?\"\n",
    "\n",
    "max_new_tokens = 64\n",
    "\n",
    "print(f\"\\nControl Model: \\n\")\n",
    "print(prompt_response(model, tokenizer, system_prompt, prompt, max_new_tokens = max_new_tokens))\n",
    "\n",
    "\n",
    "print(f\"\\nROME Testing Model: \\n\")\n",
    "print(prompt_response(testing_model, tokenizer, system_prompt, prompt, max_new_tokens = max_new_tokens))\n",
    "\n",
    "print(f\"\\nROME Testing Model With Concept Erasure Hook: \\n\")\n",
    "with erasure_hook(testing_model, cav, layer_idx=16):\n",
    "    print(prompt_response(testing_model, tokenizer, system_prompt, prompt, max_new_tokens = max_new_tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96e8358-b40c-433b-ac52-418c7ab2e5bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
