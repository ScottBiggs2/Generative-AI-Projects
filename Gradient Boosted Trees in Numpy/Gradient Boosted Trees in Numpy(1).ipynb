{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e064fd52-2f68-4f6a-a3f9-71bd1f7c685b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.datasets import fetch_california_housing, load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\n",
    "\n",
    "from collections import deque\n",
    "from sklearn.base import BaseEstimator, RegressorMixin, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "\n",
    "import xgboost\n",
    "import lightgbm\n",
    "\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "from lightgbm import LGBMRegressor, LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cec43059-dd74-43b1-9c3f-baa9f0e7fdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeRegressor:\n",
    "    def __init__(self, max_depth: int = 3):\n",
    "        \"\"\"\n",
    "        A simple decision tree regressor that splits to minimize variance.\n",
    "\n",
    "        Args:\n",
    "            max_depth (int): Maximum depth of the tree.\n",
    "        \"\"\"\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "\n",
    "    def _find_best_split(self, X: np.ndarray, y: np.ndarray) -> tuple:\n",
    "        \"\"\"\n",
    "        Find the best feature and threshold to split the data by minimizing variance.\n",
    "\n",
    "        Args:\n",
    "            X (np.ndarray): Feature matrix of shape (n_samples, n_features).\n",
    "            y (np.ndarray): Target vector of shape (n_samples,).\n",
    "\n",
    "        Returns:\n",
    "            tuple: (best_feature_index, best_threshold)\n",
    "        \"\"\"\n",
    "        best_feature, best_threshold, best_mse = None, None, float('inf')\n",
    "\n",
    "        for feature in range(X.shape[1]):\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            for threshold in thresholds:\n",
    "                left_mask = X[:, feature] <= threshold\n",
    "                left_y, right_y = y[left_mask], y[~left_mask]\n",
    "\n",
    "                if len(left_y) == 0 or len(right_y) == 0:\n",
    "                    continue\n",
    "\n",
    "                # Weighted sum of variances\n",
    "                mse = np.var(left_y) * len(left_y) + np.var(right_y) * len(right_y)\n",
    "                if mse < best_mse:\n",
    "                    best_feature, best_threshold, best_mse = feature, threshold, mse\n",
    "\n",
    "        return best_feature, best_threshold\n",
    "\n",
    "    def _build_tree(self, X: np.ndarray, y: np.ndarray, depth: int = 0):\n",
    "        \"\"\"\n",
    "        Recursively build the regression tree.\n",
    "\n",
    "        Args:\n",
    "            X (np.ndarray): Feature matrix.\n",
    "            y (np.ndarray): Target values.\n",
    "            depth (int): Current tree depth.\n",
    "\n",
    "        Returns:\n",
    "            Tree node or prediction value.\n",
    "        \"\"\"\n",
    "        if depth >= self.max_depth or len(y) < 2:\n",
    "            return np.mean(y)\n",
    "\n",
    "        feature, threshold = self._find_best_split(X, y)\n",
    "        if feature is None:\n",
    "            return np.mean(y)\n",
    "\n",
    "        left_mask = X[:, feature] <= threshold\n",
    "        left_tree = self._build_tree(X[left_mask], y[left_mask], depth + 1)\n",
    "        right_tree = self._build_tree(X[~left_mask], y[~left_mask], depth + 1)\n",
    "\n",
    "        return (feature, threshold, left_tree, right_tree)\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Train the decision tree regressor.\n",
    "\n",
    "        Args:\n",
    "            X (np.ndarray): Feature matrix.\n",
    "            y (np.ndarray): Target vector.\n",
    "        \"\"\"\n",
    "        self.tree = self._build_tree(X, y)\n",
    "\n",
    "    def _predict_sample(self, x: np.ndarray, tree) -> float:\n",
    "        \"\"\"\n",
    "        Predict the target value for a single sample.\n",
    "\n",
    "        Args:\n",
    "            x (np.ndarray): A single sample.\n",
    "            tree: The tree structure.\n",
    "\n",
    "        Returns:\n",
    "            float: Predicted value.\n",
    "        \"\"\"\n",
    "        if not isinstance(tree, tuple):\n",
    "            return tree\n",
    "        feature, threshold, left_tree, right_tree = tree\n",
    "        subtree = left_tree if x[feature] <= threshold else right_tree\n",
    "        return self._predict_sample(x, subtree)\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Predict target values for input samples.\n",
    "\n",
    "        Args:\n",
    "            X (np.ndarray): Feature matrix.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Predicted values.\n",
    "        \"\"\"\n",
    "        return np.array([self._predict_sample(x, self.tree) for x in X])\n",
    "\n",
    "\n",
    "class CustomGBDTBase(BaseEstimator):\n",
    "    def __init__(self, n_estimators: int = 100, learning_rate: float = 0.1, max_depth: int = 3):\n",
    "        \"\"\"\n",
    "        Base class for Gradient Boosted Decision Trees.\n",
    "\n",
    "        Args:\n",
    "            n_estimators (int): Number of boosting rounds.\n",
    "            learning_rate (float): Step size shrinkage.\n",
    "            max_depth (int): Maximum depth of each regression tree.\n",
    "        \"\"\"\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "\n",
    "    def _create_tree(self) -> DecisionTreeRegressor:\n",
    "        \"\"\"Factory method to create a new regression tree.\"\"\"\n",
    "        return DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "\n",
    "    def _initial_prediction(self, y: np.ndarray) -> float:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _calculate_gradient(self, y: np.ndarray, predictions: np.ndarray) -> np.ndarray:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _transform_output(self, raw_predictions: np.ndarray) -> np.ndarray:\n",
    "        return raw_predictions\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Fit the gradient boosting model.\n",
    "\n",
    "        Args:\n",
    "            X (np.ndarray): Training features.\n",
    "            y (np.ndarray): Training targets.\n",
    "        \"\"\"\n",
    "        self.base_pred = self._initial_prediction(y)\n",
    "        predictions = np.full(y.shape, self.base_pred)\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            residuals = self._calculate_gradient(y, predictions)\n",
    "            tree = self._create_tree()\n",
    "            tree.fit(X, residuals)\n",
    "            predictions += self.learning_rate * tree.predict(X)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Predict using the gradient boosted ensemble.\n",
    "\n",
    "        Args:\n",
    "            X (np.ndarray): Input features.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Model predictions.\n",
    "        \"\"\"\n",
    "        raw_preds = np.full(X.shape[0], self.base_pred)\n",
    "        for tree in self.trees:\n",
    "            raw_preds += self.learning_rate * tree.predict(X)\n",
    "        return self._transform_output(raw_preds)\n",
    "\n",
    "\n",
    "class CustomGBR(CustomGBDTBase):\n",
    "    \"\"\"Custom Gradient Boosted Regressor.\"\"\"\n",
    "\n",
    "    def _initial_prediction(self, y: np.ndarray) -> float:\n",
    "        return np.mean(y)\n",
    "\n",
    "    def _calculate_gradient(self, y: np.ndarray, predictions: np.ndarray) -> np.ndarray:\n",
    "        return y - predictions  # Negative gradient for MSE\n",
    "\n",
    "    def _transform_output(self, raw_predictions: np.ndarray) -> np.ndarray:\n",
    "        return raw_predictions\n",
    "\n",
    "\n",
    "class CustomGBC(CustomGBDTBase):\n",
    "    \"\"\"Custom Gradient Boosted Classifier.\"\"\"\n",
    "\n",
    "    def _initial_prediction(self, y: np.ndarray) -> float:\n",
    "        pos = np.mean(y)\n",
    "        epsilon = 1e-8\n",
    "        return np.log((pos + epsilon) / (1 - pos + epsilon))  # Log-odds\n",
    "\n",
    "    def _calculate_gradient(self, y: np.ndarray, predictions: np.ndarray) -> np.ndarray:\n",
    "        p = 1 / (1 + np.exp(-predictions))  # Sigmoid\n",
    "        return y - p  # Negative log-loss gradient\n",
    "\n",
    "    def _transform_output(self, raw_predictions: np.ndarray) -> np.ndarray:\n",
    "        probs = 1 / (1 + np.exp(-raw_predictions))\n",
    "        return (probs >= 0.5).astype(int)\n",
    "\n",
    "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
    "        raw_preds = np.full(X.shape[0], self.base_pred)\n",
    "        for tree in self.trees:\n",
    "            raw_preds += self.learning_rate * tree.predict(X)\n",
    "        probs = 1 / (1 + np.exp(-raw_preds))\n",
    "        return np.vstack([1 - probs, probs]).T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56c37f8-df7d-4813-a868-2d7bfbdf8934",
   "metadata": {},
   "source": [
    "# Optimizations: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3242ca42-791a-470c-a456-a50643ea348d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedDecisionTree:\n",
    "    \"\"\"Histogram-based decision tree with proper node indexing\"\"\"\n",
    "    def __init__(self, max_depth=3, min_samples_split=2, bins=128):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.bins = bins\n",
    "        self.nodes = []\n",
    "        self.root = None\n",
    "        \n",
    "    def _find_best_split(self, X, y):\n",
    "        \"\"\"Find best split using histogram approximation\"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        best_feature, best_threshold, best_mse = None, None, float('inf')\n",
    "        \n",
    "        for f in range(n_features):\n",
    "            # Create histogram for this feature\n",
    "            hist, edges = np.histogram(X[:, f], bins=self.bins)\n",
    "            n_bins = len(hist)\n",
    "            bin_indices = np.digitize(X[:, f], edges)\n",
    "            \n",
    "            # Precompute bin statistics\n",
    "            bin_sums = np.zeros(n_bins)\n",
    "            bin_counts = np.zeros(n_bins)\n",
    "            \n",
    "            for i in range(n_samples):\n",
    "                bin_idx = bin_indices[i] - 1\n",
    "                if bin_idx >= n_bins:\n",
    "                    bin_idx = n_bins - 1\n",
    "                bin_sums[bin_idx] += y[i]\n",
    "                bin_counts[bin_idx] += 1\n",
    "            \n",
    "            # Find best split in histogram space\n",
    "            cumsum_left = 0\n",
    "            count_left = 0\n",
    "            \n",
    "            for i in range(n_bins - 1):\n",
    "                # Update left statistics\n",
    "                cumsum_left += bin_sums[i]\n",
    "                count_left += bin_counts[i]\n",
    "                \n",
    "                # Skip if not enough samples\n",
    "                if count_left < self.min_samples_split or (n_samples - count_left) < self.min_samples_split:\n",
    "                    continue\n",
    "                \n",
    "                # Compute right statistics\n",
    "                cumsum_right = np.sum(bin_sums) - cumsum_left\n",
    "                count_right = n_samples - count_left\n",
    "                \n",
    "                # Compute MSE\n",
    "                mse_left = (np.sum(bin_sums[i:]**2 / np.maximum(bin_counts[i:], 1)) - cumsum_left**2 / count_left)\n",
    "                mse_right = (np.sum(bin_sums[:i]**2 / np.maximum(bin_counts[:i], 1)) - cumsum_right**2 / count_right)\n",
    "                mse_total = mse_left + mse_right\n",
    "                \n",
    "                if mse_total < best_mse:\n",
    "                    best_mse = mse_total\n",
    "                    best_feature = f\n",
    "                    best_threshold = edges[i+1]\n",
    "        \n",
    "        return best_feature, best_threshold, best_mse\n",
    "    \n",
    "    def _build_tree(self, X, y):\n",
    "        \"\"\"Build tree using BFS with proper node indexing\"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        root = {\n",
    "            'id': 0,\n",
    "            'indices': np.arange(n_samples),\n",
    "            'depth': 0,\n",
    "            'is_leaf': False,\n",
    "            'value': np.mean(y)\n",
    "        }\n",
    "        self.nodes = [root]\n",
    "        queue = deque([root])\n",
    "        next_id = 1\n",
    "        \n",
    "        while queue:\n",
    "            node = queue.popleft()\n",
    "            indices = node['indices']\n",
    "            X_node = X[indices]\n",
    "            y_node = y[indices]\n",
    "            \n",
    "            # Check stopping criteria\n",
    "            if (node['depth'] >= self.max_depth or \n",
    "                len(y_node) < self.min_samples_split or\n",
    "                np.var(y_node) < 1e-6):\n",
    "                node['is_leaf'] = True\n",
    "                continue\n",
    "                \n",
    "            # Find best split\n",
    "            feature, threshold, mse = self._find_best_split(X_node, y_node)\n",
    "            \n",
    "            if feature is None:\n",
    "                node['is_leaf'] = True\n",
    "                continue\n",
    "                \n",
    "            # Split data\n",
    "            left_mask = X_node[:, feature] <= threshold\n",
    "            right_mask = ~left_mask\n",
    "            \n",
    "            # Create child nodes\n",
    "            left_child = {\n",
    "                'id': next_id,\n",
    "                'indices': indices[left_mask],\n",
    "                'depth': node['depth'] + 1,\n",
    "                'is_leaf': False,\n",
    "                'value': np.mean(y_node[left_mask])\n",
    "            }\n",
    "            next_id += 1\n",
    "            \n",
    "            right_child = {\n",
    "                'id': next_id,\n",
    "                'indices': indices[right_mask],\n",
    "                'depth': node['depth'] + 1,\n",
    "                'is_leaf': False,\n",
    "                'value': np.mean(y_node[right_mask])\n",
    "            }\n",
    "            next_id += 1\n",
    "            \n",
    "            # Update current node\n",
    "            node['feature'] = feature\n",
    "            node['threshold'] = threshold\n",
    "            node['left'] = left_child\n",
    "            node['right'] = right_child\n",
    "            node['is_leaf'] = False\n",
    "            \n",
    "            # Add children to queue and node list\n",
    "            self.nodes.extend([left_child, right_child])\n",
    "            queue.append(left_child)\n",
    "            queue.append(right_child)\n",
    "        \n",
    "        return root\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.root = self._build_tree(X, y)\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"Vectorized prediction using tree traversal\"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        preds = np.zeros(n_samples)\n",
    "        current_nodes = [self.root] * n_samples\n",
    "        \n",
    "        # Process nodes until all are leaves\n",
    "        while any(not node.get('is_leaf', True) for node in current_nodes):\n",
    "            # Process each sample\n",
    "            for i in range(n_samples):\n",
    "                node = current_nodes[i]\n",
    "                \n",
    "                if node.get('is_leaf', True):\n",
    "                    # Already at leaf, store prediction\n",
    "                    preds[i] = node.get('value', 0)\n",
    "                    continue\n",
    "                    \n",
    "                # Traverse to next node\n",
    "                if X[i, node['feature']] <= node['threshold']:\n",
    "                    current_nodes[i] = node['left']\n",
    "                else:\n",
    "                    current_nodes[i] = node['right']\n",
    "        \n",
    "        # Handle any remaining non-leaf nodes\n",
    "        for i in range(n_samples):\n",
    "            if not current_nodes[i].get('is_leaf', False):\n",
    "                preds[i] = current_nodes[i].get('value', 0)\n",
    "                \n",
    "        return preds\n",
    "\n",
    "\n",
    "class OptimizedGradientBoostingRegressor(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"Optimized Gradient Boosting for Regression\"\"\"\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3, \n",
    "                 min_samples_split=2, bins=128, subsample=0.8, verbose=0):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.bins = bins\n",
    "        self.subsample = subsample\n",
    "        self.verbose = verbose\n",
    "        self.trees = []\n",
    "        self.base_pred = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        X, y = check_X_y(X, y)\n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        # Initial prediction\n",
    "        self.base_pred = np.mean(y)\n",
    "        predictions = np.full(n_samples, self.base_pred)\n",
    "        \n",
    "        for i in range(self.n_estimators):\n",
    "            # Compute residuals\n",
    "            residuals = y - predictions\n",
    "            \n",
    "            # Create and fit tree\n",
    "            tree = OptimizedDecisionTree(\n",
    "                max_depth=self.max_depth,\n",
    "                min_samples_split=self.min_samples_split,\n",
    "                bins=self.bins\n",
    "            )\n",
    "            \n",
    "            # Subsample data\n",
    "            if self.subsample < 1.0:\n",
    "                idx = np.random.choice(n_samples, int(n_samples * self.subsample), replace=False)\n",
    "                tree.fit(X[idx], residuals[idx])\n",
    "            else:\n",
    "                tree.fit(X, residuals)\n",
    "            \n",
    "            # Update predictions\n",
    "            tree_pred = tree.predict(X)\n",
    "            predictions += self.learning_rate * tree_pred\n",
    "            \n",
    "            # Store tree\n",
    "            self.trees.append(tree)\n",
    "            \n",
    "            # Progress logging\n",
    "            if self.verbose and (i+1) % 10 == 0:\n",
    "                mse = mean_squared_error(y, predictions)\n",
    "                print(f\"Tree {i+1}/{self.n_estimators} - MSE: {mse:.4f}\")\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        # check_is_fitted(self)\n",
    "        # X = check_array(X)\n",
    "        preds = np.full(X.shape[0], self.base_pred)\n",
    "        \n",
    "        for tree in self.trees:\n",
    "            preds += self.learning_rate * tree.predict(X)\n",
    "            \n",
    "        return preds\n",
    "\n",
    "\n",
    "class OptimizedGradientBoostingClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"Optimized Gradient Boosting for Classification\"\"\"\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3, \n",
    "                 min_samples_split=2, bins=128, subsample=0.8, verbose=0):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.bins = bins\n",
    "        self.subsample = subsample\n",
    "        self.verbose = verbose\n",
    "        self.trees = []\n",
    "        self.base_log_odds = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        X, y = check_X_y(X, y)\n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        # Convert to binary classification\n",
    "        self.classes_, y = np.unique(y, return_inverse=True)\n",
    "        y = y.astype(float)\n",
    "        \n",
    "        # Initial prediction (log-odds)\n",
    "        pos = np.mean(y)\n",
    "        epsilon = 1e-8\n",
    "        self.base_log_odds = np.log((pos + epsilon) / (1 - pos + epsilon))\n",
    "        log_odds = np.full(n_samples, self.base_log_odds)\n",
    "        probabilities = 1 / (1 + np.exp(-log_odds))\n",
    "        \n",
    "        for i in range(self.n_estimators):\n",
    "            # Compute gradients\n",
    "            gradients = y - probabilities\n",
    "            \n",
    "            # Create and fit tree\n",
    "            tree = OptimizedDecisionTree(\n",
    "                max_depth=self.max_depth,\n",
    "                min_samples_split=self.min_samples_split,\n",
    "                bins=self.bins\n",
    "            )\n",
    "            \n",
    "            # Subsample data\n",
    "            if self.subsample < 1.0:\n",
    "                idx = np.random.choice(n_samples, int(n_samples * self.subsample), replace=False)\n",
    "                tree.fit(X[idx], gradients[idx])\n",
    "            else:\n",
    "                tree.fit(X, gradients)\n",
    "            \n",
    "            # Update predictions\n",
    "            tree_pred = tree.predict(X)\n",
    "            log_odds += self.learning_rate * tree_pred\n",
    "            probabilities = 1 / (1 + np.exp(-log_odds))\n",
    "            \n",
    "            # Store tree\n",
    "            self.trees.append(tree)\n",
    "            \n",
    "            # Progress logging\n",
    "            if self.verbose and (i+1) % 10 == 0:\n",
    "                acc = np.mean((probabilities > 0.5) == y)\n",
    "                print(f\"Tree {i+1}/{self.n_estimators} - Accuracy: {acc:.4f}\")\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        # check_is_fitted(self)\n",
    "        # X = check_array(X)\n",
    "        log_odds = np.full(X.shape[0], self.base_log_odds)\n",
    "        \n",
    "        for tree in self.trees:\n",
    "            log_odds += self.learning_rate * tree.predict(X)\n",
    "            \n",
    "        probabilities = 1 / (1 + np.exp(-log_odds))\n",
    "        return np.vstack([1 - probabilities, probabilities]).T\n",
    "        \n",
    "    def predict(self, X):\n",
    "        proba = self.predict_proba(X)\n",
    "        return self.classes_[(proba[:, 1] > 0.5).astype(int)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0dbfbdf6-f859-43f7-b330-a2d96c3e94a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================================================================================================\n",
      "GBDT Algorithm Comparison Summary\n",
      "========================================================================================================================\n",
      "| Model                 | Task           |        MSE |   Train Time (s) |   n_estimators |   learning_rate |   max_depth |   Accuracy |\n",
      "|:----------------------|:---------------|-----------:|-----------------:|---------------:|----------------:|------------:|-----------:|\n",
      "| Custom GBDT           | regression     |   0.293848 |     1848.16      |            100 |             0.1 |           3 | nan        |\n",
      "| Custom Optimized GBDT | regression     |   1.31063  |       19.8692    |            100 |             0.1 |           3 | nan        |\n",
      "| Scikit-learn          | regression     |   0.293997 |        2.94729   |            100 |             0.1 |           3 | nan        |\n",
      "| XGBoost               | regression     |   0.295227 |        0.0657778 |            100 |             0.1 |           3 | nan        |\n",
      "| LightGBM              | regression     |   0.289398 |        0.10589   |            100 |             0.1 |           3 | nan        |\n",
      "| Custom GBDT           | classification | nan        |       63.0144    |            100 |             0.1 |           3 |   0.95614  |\n",
      "| Custom Optimized GBDT | classification | nan        |       19.3304    |            100 |             0.1 |           3 |   0.622807 |\n",
      "| Scikit-learn          | classification | nan        |        0.311169  |            100 |             0.1 |           3 |   0.95614  |\n",
      "| XGBoost               | classification | nan        |        0.0465579 |            100 |             0.1 |           3 |   0.95614  |\n",
      "| LightGBM              | classification | nan        |        0.02404   |            100 |             0.1 |           3 |   0.95614  |\n",
      "========================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Configure experiment settings\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2\n",
    "N_ESTIMATORS = 100\n",
    "LEARNING_RATE = 0.1\n",
    "MAX_DEPTH = 3\n",
    "\n",
    "# Initialize logging\n",
    "results = []\n",
    "\n",
    "def run_experiment(model, model_name, task, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Train and evaluate a model, returning metrics and time\"\"\"\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    preds = model.predict(X_test)\n",
    "    \n",
    "    if task == \"regression\":\n",
    "        metric = mean_squared_error(y_test, preds)\n",
    "        metric_name = \"MSE\"\n",
    "    else:\n",
    "        metric = accuracy_score(y_test, preds)\n",
    "        metric_name = \"Accuracy\"\n",
    "    \n",
    "    return {\n",
    "        \"Model\": model_name,\n",
    "        \"Task\": task,\n",
    "        metric_name: metric,\n",
    "        \"Train Time (s)\": train_time,\n",
    "        \"n_estimators\": N_ESTIMATORS,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"max_depth\": MAX_DEPTH\n",
    "    }\n",
    "\n",
    "# Run regression experiment\n",
    "def regression_experiment():\n",
    "    data = fetch_california_housing()\n",
    "    X, y = data.data, data.target\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE\n",
    "    )\n",
    "    \n",
    "    # Initialize models\n",
    "    models = {\n",
    "        \"Custom GBDT\": CustomGBR(n_estimators=N_ESTIMATORS, \n",
    "                                learning_rate=LEARNING_RATE, \n",
    "                                max_depth=MAX_DEPTH),\n",
    "        \"Custom Optimized GBDT\": OptimizedGradientBoostingRegressor(n_estimators = N_ESTIMATORS,\n",
    "                                                                   learning_rate = LEARNING_RATE,\n",
    "                                                                   max_depth = MAX_DEPTH),\n",
    "        \"Scikit-learn\": GradientBoostingRegressor(n_estimators=N_ESTIMATORS, \n",
    "                                                 learning_rate=LEARNING_RATE, \n",
    "                                                 max_depth=MAX_DEPTH,\n",
    "                                                 random_state=RANDOM_STATE),\n",
    "        \"XGBoost\": XGBRegressor(n_estimators=N_ESTIMATORS, \n",
    "                               learning_rate=LEARNING_RATE, \n",
    "                               max_depth=MAX_DEPTH,\n",
    "                               random_state=RANDOM_STATE,\n",
    "                               verbosity=0),\n",
    "        \"LightGBM\": LGBMRegressor(n_estimators=N_ESTIMATORS, \n",
    "                                 learning_rate=LEARNING_RATE, \n",
    "                                 max_depth=MAX_DEPTH,\n",
    "                                 random_state=RANDOM_STATE,\n",
    "                                 verbose=-1)\n",
    "    }\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        results.append(run_experiment(\n",
    "            model, name, \"regression\", X_train, X_test, y_train, y_test\n",
    "        ))\n",
    "\n",
    "# Run classification experiment\n",
    "def classification_experiment():\n",
    "    data = load_breast_cancer()\n",
    "    X, y = data.data, data.target\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE\n",
    "    )\n",
    "    \n",
    "    # Initialize models\n",
    "    models = {\n",
    "        \"Custom GBDT\": CustomGBC(n_estimators=N_ESTIMATORS, \n",
    "                                learning_rate=LEARNING_RATE, \n",
    "                                max_depth=MAX_DEPTH),\n",
    "        \"Custom Optimized GBDT\": OptimizedGradientBoostingClassifier(n_estimators=N_ESTIMATORS,\n",
    "                                                              learning_rate = LEARNING_RATE,\n",
    "                                                              max_depth=MAX_DEPTH),\n",
    "        \"Scikit-learn\": GradientBoostingClassifier(n_estimators=N_ESTIMATORS, \n",
    "                                                 learning_rate=LEARNING_RATE, \n",
    "                                                 max_depth=MAX_DEPTH,\n",
    "                                                 random_state=RANDOM_STATE),\n",
    "        \"XGBoost\": XGBClassifier(n_estimators=N_ESTIMATORS, \n",
    "                                learning_rate=LEARNING_RATE, \n",
    "                                max_depth=MAX_DEPTH,\n",
    "                                random_state=RANDOM_STATE,\n",
    "                                verbosity=0),\n",
    "        \"LightGBM\": LGBMClassifier(n_estimators=N_ESTIMATORS, \n",
    "                                  learning_rate=LEARNING_RATE, \n",
    "                                  max_depth=MAX_DEPTH,\n",
    "                                  random_state=RANDOM_STATE,\n",
    "                                  verbose=-1)\n",
    "    }\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        results.append(run_experiment(\n",
    "            model, name, \"classification\", X_train, X_test, y_train, y_test\n",
    "        ))\n",
    "\n",
    "# Run experiments\n",
    "regression_experiment()\n",
    "classification_experiment()\n",
    "\n",
    "# Create results table\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"GBDT Algorithm Comparison Summary\")\n",
    "print(\"=\"*120)\n",
    "print(results_df.to_markdown(index=False))\n",
    "print(\"=\"*120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54064ec6-bb1c-41c3-a0e8-35f8f1a1d0be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
